{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorial Word Representations\n",
    "\n",
    "## Background\n",
    "Representing words as dense vectors over a finite-dimensional space was one of the recent breakthroughs in Natural Language Processing. Vectorial representations allow space-efficient, informationally rich storage of words that adequately captures their semantic content and enables numerical computation on them. Word vectors are the standard input representation for language-oriented machine learning architectures. Even though new methods for constructing such representations emerge frequently, the original set of published papers remain the de facto point of reference. For this assignment, you will be asked to implement a small-scale variant of one such paper, namely [Global Word Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf).\n",
    "\n",
    "Much of the code and data pre-processing has already been done for you. Additionally, notes on the paper will appear throughout the notebook to guide you along the code. It is, however, important to read and understand the paper, its terminology and the theory behind it before attempting to go through with the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics\n",
    "\n",
    "The paper's proposed model, GloVe, aims to densely represent words in a way that captures the global corpus statistics. \n",
    "\n",
    "The construction it encodes is the word __co-occurrence matrix__. A co-occurrence matrix is a data structure that counts the amount of times each word has appeared within the context of each other word. The definition of a context varies; usually, context is implied to be a fixed-length span (that may or may not be allowed to escape sentence boundaries) around a word. \n",
    "\n",
    "For instance, in the sentence below and for a context length of 2, the word <span style=\"color:pink\">__Earth__</span> occurs in the context of <span style=\"color:lightgreen\">made</span> (1), <span style=\"color:lightgreen\">on</span> (1), <span style=\"color:lightgreen\">as</span> (1), <span style=\"color:lightgreen\">an</span> (1).\n",
    "\n",
    "> \"He struck most of the friends he had <span style=\"color:lightgreen\">made on</span> <span style=\"color:pink\">__Earth__</span> <span style=\"color:lightgreen\">as an</span> eccentric\"\n",
    "\n",
    "Similarly, the word <span style=\"color:pink\">__friends__</span> occurs in the context of <span style=\"color:lightgreen\">of</span> (1), <span style=\"color:lightgreen\">the</span> (1), <span style=\"color:lightgreen\">he</span> (1), <span style=\"color:lightgreen\">had</span> (1).\n",
    "\n",
    "> \"He struck most <span style=\"color:lightgreen\">of the</span> <span style=\"color:pink\">__friends__</span> <span style=\"color:lightgreen\">he had</span> made on Earth as an eccentric\"\n",
    "\n",
    "An alternative definition of a context would be, for instance, the variable-length windows spanned by a full sentence.\n",
    "\n",
    "Contexts may be summed across sentences or entire corpora; the summed context of <span style=\"color:pink\">he</span> in the example sentence is: <span style=\"color:lightgreen\">struck</span> (1), <span style=\"color:lightgreen\">most</span> (1), <span style=\"color:lightgreen\">the</span> (1), <span style=\"color:lightgreen\">friends</span> (1), <span style=\"color:lightgreen\">had</span> (1), <span style=\"color:lightgreen\">made</span> (1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this assignment, we have already prepared a co-occurrence matrix for you from a minimally processed version of the *Harry Potter* books. The pickle file contains three items:\n",
    "1. `vocab`: a dictionary mapping words to unique ids, containing $N$ unique words\n",
    "1. `contexts`: a dictionary mapping words to their contexts, where contexts are themselves dicts from words to ints\n",
    "2. `X`: a torch LongTensor $\\mathbf{X}$ of size $N \\times N$, where $\\mathbf{X}[i,j]$ denotes the number of times the word with id $j$ has appeared in the context of the word with id $i$\n",
    "\n",
    "Extremely common or uncommon words (i.e. words with too few or too many global occurrences) have been filtered out for practical reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from typing import Dict, Callable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5359\n"
     ]
    }
   ],
   "source": [
    "with open('output.p', 'rb') as f:\n",
    "    vocab, contexts, X = pickle.load(f)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the summed context of the word 'portrait'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hole', 80),\n",
       " ('harry', 44),\n",
       " ('said', 31),\n",
       " ('fat', 30),\n",
       " ('lady', 26),\n",
       " ('hermione', 20),\n",
       " ('room', 20),\n",
       " ('climbed', 18),\n",
       " ('common', 17),\n",
       " ('ron', 16),\n",
       " ('dumbledore', 16),\n",
       " ('phineas', 15),\n",
       " ('swung', 14),\n",
       " ('open', 14),\n",
       " ('t', 13),\n",
       " ('sirius', 11),\n",
       " ('nigellus', 11),\n",
       " ('reached', 10),\n",
       " ('just', 9),\n",
       " ('time', 9),\n",
       " ('forward', 8),\n",
       " ('like', 8),\n",
       " ('mother', 8),\n",
       " ('little', 8),\n",
       " ('place', 8),\n",
       " ('black', 7),\n",
       " ('pushed', 7),\n",
       " ('gryffindor', 7),\n",
       " ('professor', 7),\n",
       " ('corridor', 6),\n",
       " ('voice', 6),\n",
       " ('got', 6),\n",
       " ('visit', 6),\n",
       " ('moment', 6),\n",
       " ('really', 6),\n",
       " ('good', 6),\n",
       " ('come', 5),\n",
       " ('turned', 5),\n",
       " ('thing', 5),\n",
       " ('let', 5),\n",
       " ('talking', 5),\n",
       " ('walked', 5),\n",
       " ('entrance', 5),\n",
       " ('head', 5),\n",
       " ('hurried', 5),\n",
       " ('came', 5),\n",
       " ('ve', 5),\n",
       " ('mcgonagall', 5),\n",
       " ('passed', 5),\n",
       " ('way', 5),\n",
       " ('wizard', 5),\n",
       " ('look', 5),\n",
       " ('headmaster', 5),\n",
       " ('reveal', 4),\n",
       " ('chair', 4),\n",
       " ('seventh', 4),\n",
       " ('wall', 4),\n",
       " ('snape', 4),\n",
       " ('castle', 4),\n",
       " ('past', 4),\n",
       " ('headed', 4),\n",
       " ('fred', 4),\n",
       " ('know', 4),\n",
       " ('canvas', 4),\n",
       " ('opened', 4),\n",
       " ('stood', 4),\n",
       " ('long', 4),\n",
       " ('frame', 4),\n",
       " ('dinner', 4),\n",
       " ('hall', 4),\n",
       " ('doing', 4),\n",
       " ('heard', 4),\n",
       " ('grimmauld', 4),\n",
       " ('minister', 4),\n",
       " ('fudge', 4),\n",
       " ('hung', 3),\n",
       " ('pink', 3),\n",
       " ('percy', 3),\n",
       " ('going', 3),\n",
       " ('stop', 3),\n",
       " ('floor', 3),\n",
       " ('snout', 3),\n",
       " ('scrambled', 3),\n",
       " ('hurrying', 3),\n",
       " ('stand', 3),\n",
       " ('fight', 3),\n",
       " ('point', 3),\n",
       " ('waiting', 3),\n",
       " ('colin', 3),\n",
       " ('left', 3),\n",
       " ('closing', 3),\n",
       " ('climbing', 3),\n",
       " ('tower', 3),\n",
       " ('large', 3),\n",
       " ('away', 3),\n",
       " ('right', 3),\n",
       " ('minutes', 3),\n",
       " ('closed', 3),\n",
       " ('later', 3),\n",
       " ('vanished', 3),\n",
       " ('sir', 3),\n",
       " ('cadogan', 3),\n",
       " ('mad', 3),\n",
       " ('entered', 3),\n",
       " ('pulled', 3),\n",
       " ('house', 3),\n",
       " ('hagrid', 3),\n",
       " ('clambered', 3),\n",
       " ('realized', 3),\n",
       " ('asked', 3),\n",
       " ('looking', 3),\n",
       " ('barely', 3),\n",
       " ('painting', 3),\n",
       " ('walk', 3),\n",
       " ('prime', 3),\n",
       " ('ugly', 3),\n",
       " ('man', 3),\n",
       " ('romilda', 3),\n",
       " ('vane', 3),\n",
       " ('approached', 3),\n",
       " ('ariana', 3),\n",
       " ('figures', 3),\n",
       " ('prefects', 2),\n",
       " ('silk', 2),\n",
       " ('armchairs', 2),\n",
       " ('spoke', 2),\n",
       " ('followed', 2),\n",
       " ('tomorrow', 2),\n",
       " ('didn', 2),\n",
       " ('herself', 2),\n",
       " ('pig', 2),\n",
       " ('dormitory', 2),\n",
       " ('stairs', 2),\n",
       " ('managed', 2),\n",
       " ('climb', 2),\n",
       " ('neville', 2),\n",
       " ('impatiently', 2),\n",
       " ('arms', 2),\n",
       " ('held', 2),\n",
       " ('winking', 2),\n",
       " ('justin', 2),\n",
       " ('gryffindors', 2),\n",
       " ('cloak', 2),\n",
       " ('hidden', 2),\n",
       " ('girls', 2),\n",
       " ('need', 2),\n",
       " ('crowd', 2),\n",
       " ('heads', 2),\n",
       " ('moving', 2),\n",
       " ('able', 2),\n",
       " ('taken', 2),\n",
       " ('thought', 2),\n",
       " ('firmly', 2),\n",
       " ('dormitories', 2),\n",
       " ('simply', 2),\n",
       " ('carried', 2),\n",
       " ('ask', 2),\n",
       " ('outside', 2),\n",
       " ('face', 2),\n",
       " ('landing', 2),\n",
       " ('met', 2),\n",
       " ('sight', 2),\n",
       " ('concealed', 2),\n",
       " ('george', 2),\n",
       " ('downstairs', 2),\n",
       " ('told', 2),\n",
       " ('bed', 2),\n",
       " ('gone', 2),\n",
       " ('silent', 2),\n",
       " ('noise', 2),\n",
       " ('ears', 2),\n",
       " ('crept', 2),\n",
       " ('waited', 2),\n",
       " ('laugh', 2),\n",
       " ('think', 2),\n",
       " ('seen', 2),\n",
       " ('lavender', 2),\n",
       " ('packed', 2),\n",
       " ('picture', 2),\n",
       " ('yell', 2),\n",
       " ('life', 2),\n",
       " ('wake', 2),\n",
       " ('added', 2),\n",
       " ('curtains', 2),\n",
       " ('himself', 2),\n",
       " ('idea', 2),\n",
       " ('mrs', 2),\n",
       " ('did', 2),\n",
       " ('correct', 2),\n",
       " ('door', 2),\n",
       " ('parchment', 2),\n",
       " ('appeared', 2),\n",
       " ('covered', 2),\n",
       " ('ran', 2),\n",
       " ('view', 2),\n",
       " ('eyes', 2),\n",
       " ('study', 2),\n",
       " ('sideways', 2),\n",
       " ('queue', 2),\n",
       " ('middle', 2),\n",
       " ('looked', 2),\n",
       " ('dark', 2),\n",
       " ('standing', 2),\n",
       " ('red', 2),\n",
       " ('broken', 2),\n",
       " ('dippet', 2),\n",
       " ('calling', 2),\n",
       " ('hurtled', 2),\n",
       " ('tonight', 2),\n",
       " ('felt', 2),\n",
       " ('announced', 2),\n",
       " ('arrival', 2),\n",
       " ('office', 2),\n",
       " ('boy', 2),\n",
       " ('joined', 2),\n",
       " ('took', 2),\n",
       " ('cried', 2),\n",
       " ('ginny', 2),\n",
       " ('help', 2),\n",
       " ('couldn', 2),\n",
       " ('people', 2),\n",
       " ('grounds', 2),\n",
       " ('folded', 2),\n",
       " ('saw', 2),\n",
       " ('dean', 2),\n",
       " ('died', 2),\n",
       " ('don', 2),\n",
       " ('bag', 2),\n",
       " ('fang', 2),\n",
       " ('galloping', 2),\n",
       " ('painted', 2),\n",
       " ('severus', 2),\n",
       " ('listen', 1),\n",
       " ('end', 1),\n",
       " ('woman', 1),\n",
       " ('round', 1),\n",
       " ('hunched', 1),\n",
       " ('shadows', 1),\n",
       " ('easily', 1),\n",
       " ('hissing', 1),\n",
       " ('angry', 1),\n",
       " ('inside', 1),\n",
       " ('running', 1),\n",
       " ('panted', 1),\n",
       " ('locked', 1),\n",
       " ('reminded', 1),\n",
       " ('toppled', 1),\n",
       " ('guess', 1),\n",
       " ('legs', 1),\n",
       " ('stuck', 1),\n",
       " ('clock', 1),\n",
       " ('midnight', 1),\n",
       " ('burst', 1),\n",
       " ('tail', 1),\n",
       " ('words', 1),\n",
       " ('cut', 1),\n",
       " ('short', 1),\n",
       " ('arrive', 1),\n",
       " ('pull', 1),\n",
       " ('lockhart', 1),\n",
       " ('beaming', 1),\n",
       " ('double', 1),\n",
       " ('better', 1),\n",
       " ('sign', 1),\n",
       " ('copy', 1),\n",
       " ('trolls', 1),\n",
       " ('pointing', 1),\n",
       " ('gilderoy', 1),\n",
       " ('nimbus', 1),\n",
       " ('thousand', 1),\n",
       " ('shoulder', 1),\n",
       " ('clatter', 1),\n",
       " ('creevey', 1),\n",
       " ('m', 1),\n",
       " ('hurry', 1),\n",
       " ('quidditch', 1),\n",
       " ('practice', 1),\n",
       " ('oh', 1),\n",
       " ('wow', 1),\n",
       " ('wait', 1),\n",
       " ('important', 1),\n",
       " ('wondering', 1),\n",
       " ('somewhat', 1),\n",
       " ('awkwardly', 1),\n",
       " ('began', 1),\n",
       " ('seizing', 1),\n",
       " ('throwing', 1),\n",
       " ('difficult', 1),\n",
       " ('journey', 1),\n",
       " ('crossed', 1),\n",
       " ('darkness', 1),\n",
       " ('falling', 1),\n",
       " ('dress', 1),\n",
       " ('longbottom', 1),\n",
       " ('sadly', 1),\n",
       " ('trouble', 1),\n",
       " ('remembering', 1),\n",
       " ('dementors', 1),\n",
       " ('things', 1),\n",
       " ('glad', 1),\n",
       " ('meet', 1),\n",
       " ('anybody', 1),\n",
       " ('excellent', 1),\n",
       " ('zonko', 1),\n",
       " ('work', 1),\n",
       " ('choice', 1),\n",
       " ('turn', 1),\n",
       " ('waking', 1),\n",
       " ('feast', 1),\n",
       " ('starting', 1),\n",
       " ('discussing', 1),\n",
       " ('ended', 1),\n",
       " ('jammed', 1),\n",
       " ('students', 1),\n",
       " ('curiously', 1),\n",
       " ('peered', 1),\n",
       " ('sweeping', 1),\n",
       " ('squeezed', 1),\n",
       " ('arm', 1),\n",
       " ('slashed', 1),\n",
       " ('ripped', 1),\n",
       " ('replaced', 1),\n",
       " ('gray', 1),\n",
       " ('breakfast', 1),\n",
       " ('cloaks', 1),\n",
       " ('set', 1),\n",
       " ('yellow', 1),\n",
       " ('men', 1),\n",
       " ('enjoying', 1),\n",
       " ('sat', 1),\n",
       " ('angle', 1),\n",
       " ('heel', 1),\n",
       " ('firebolt', 1),\n",
       " ('telling', 1),\n",
       " ('slammed', 1),\n",
       " ('stared', 1),\n",
       " ('ridiculous', 1),\n",
       " ('weasley', 1),\n",
       " ('possibly', 1),\n",
       " ('gotten', 1),\n",
       " ('suspiciously', 1),\n",
       " ('went', 1),\n",
       " ('paper', 1),\n",
       " ('stunned', 1),\n",
       " ('mouse', 1),\n",
       " ('holes', 1),\n",
       " ('fired', 1),\n",
       " ('lonely', 1),\n",
       " ('thoughtfully', 1),\n",
       " ('d', 1),\n",
       " ('kill', 1),\n",
       " ('teachers', 1),\n",
       " ('password', 1),\n",
       " ('freedom', 1),\n",
       " ('hasn', 1),\n",
       " ('prefect', 1),\n",
       " ('feels', 1),\n",
       " ('completely', 1),\n",
       " ('normal', 1),\n",
       " ('blast', 1),\n",
       " ('knocked', 1),\n",
       " ('backward', 1),\n",
       " ('frantically', 1),\n",
       " ('join', 1),\n",
       " ('resolutely', 1),\n",
       " ('potter', 1),\n",
       " ('minute', 1),\n",
       " ('keeping', 1),\n",
       " ('gasped', 1),\n",
       " ('say', 1),\n",
       " ('muttered', 1),\n",
       " ('sleepily', 1),\n",
       " ('weren', 1),\n",
       " ('bowed', 1),\n",
       " ('parvati', 1),\n",
       " ('winked', 1),\n",
       " ('friend', 1),\n",
       " ('night', 1),\n",
       " ('shown', 1),\n",
       " ('dragons', 1),\n",
       " ('bye', 1),\n",
       " ('tortured', 1),\n",
       " ('size', 1),\n",
       " ('unpleasant', 1),\n",
       " ('quick', 1),\n",
       " ('lupin', 1),\n",
       " ('calm', 1),\n",
       " ('returned', 1),\n",
       " ('obviously', 1),\n",
       " ('shut', 1),\n",
       " ('stopped', 1),\n",
       " ('screaming', 1),\n",
       " ('taking', 1),\n",
       " ('orders', 1),\n",
       " ('foul', 1),\n",
       " ('parents', 1),\n",
       " ('use', 1),\n",
       " ('scowled', 1),\n",
       " ('sighed', 1),\n",
       " ('dragging', 1),\n",
       " ('trunk', 1),\n",
       " ('howling', 1),\n",
       " ('rage', 1),\n",
       " ('bothering', 1),\n",
       " ('close', 1),\n",
       " ('halt', 1),\n",
       " ('alarmed', 1),\n",
       " ('kind', 1),\n",
       " ('kept', 1),\n",
       " ('sense', 1),\n",
       " ('carefully', 1),\n",
       " ('owlery', 1),\n",
       " ('hour', 1),\n",
       " ('fair', 1),\n",
       " ('giggling', 1),\n",
       " ('madly', 1),\n",
       " ('senses', 1),\n",
       " ('pale', 1),\n",
       " ('snow', 1),\n",
       " ('hair', 1),\n",
       " ('clicked', 1),\n",
       " ('tongue', 1),\n",
       " ('hastily', 1),\n",
       " ('spiral', 1),\n",
       " ('called', 1),\n",
       " ('reappeared', 1),\n",
       " ('panting', 1),\n",
       " ('slightly', 1),\n",
       " ('news', 1),\n",
       " ('blood', 1),\n",
       " ('st', 1),\n",
       " ('mungo', 1),\n",
       " ('looks', 1),\n",
       " ('bad', 1),\n",
       " ('thank', 1),\n",
       " ('rest', 1),\n",
       " ('marched', 1),\n",
       " ('clever', 1),\n",
       " ('wide', 1),\n",
       " ('message', 1),\n",
       " ('family', 1),\n",
       " ('knows', 1),\n",
       " ('destroy', 1),\n",
       " ('bored', 1),\n",
       " ('disappeared', 1),\n",
       " ('unless', 1),\n",
       " ('qualified', 1),\n",
       " ('healer', 1),\n",
       " ('witch', 1),\n",
       " ('silver', 1),\n",
       " ('gave', 1),\n",
       " ('tiny', 1),\n",
       " ('young', 1),\n",
       " ('ceiling', 1),\n",
       " ('walls', 1),\n",
       " ('oak', 1),\n",
       " ('vicious', 1),\n",
       " ('pace', 1),\n",
       " ('beds', 1),\n",
       " ('brain', 1),\n",
       " ('questions', 1),\n",
       " ('leaning', 1),\n",
       " ('watching', 1),\n",
       " ('outline', 1),\n",
       " ('occurred', 1),\n",
       " ('probably', 1),\n",
       " ('pile', 1),\n",
       " ('dropping', 1),\n",
       " ('whisper', 1),\n",
       " ('passing', 1),\n",
       " ('says', 1),\n",
       " ('roared', 1),\n",
       " ('nosed', 1),\n",
       " ('unicorns', 1),\n",
       " ('boys', 1),\n",
       " ('arguing', 1),\n",
       " ('occasional', 1),\n",
       " ('grunt', 1),\n",
       " ('sleeping', 1),\n",
       " ('surroundings', 1),\n",
       " ('reflected', 1),\n",
       " ('feelings', 1),\n",
       " ('pictures', 1),\n",
       " ('yells', 1),\n",
       " ('anger', 1),\n",
       " ('fright', 1),\n",
       " ('marching', 1),\n",
       " ('knew', 1),\n",
       " ('noticing', 1),\n",
       " ('ignoring', 1),\n",
       " ('telephone', 1),\n",
       " ('heart', 1),\n",
       " ('sank', 1),\n",
       " ('magic', 1),\n",
       " ('caused', 1),\n",
       " ('utterly', 1),\n",
       " ('terrified', 1),\n",
       " ('self', 1),\n",
       " ('delighted', 1),\n",
       " ('private', 1),\n",
       " ('proved', 1),\n",
       " ('impossible', 1),\n",
       " ('remove', 1),\n",
       " ('busy', 1),\n",
       " ('wearing', 1),\n",
       " ('quill', 1),\n",
       " ('catching', 1),\n",
       " ('eye', 1),\n",
       " ('finishing', 1),\n",
       " ('clearly', 1),\n",
       " ('silence', 1),\n",
       " ('immediately', 1),\n",
       " ('suddenly', 1),\n",
       " ('official', 1),\n",
       " ('pureblood', 1),\n",
       " ('vivid', 1),\n",
       " ('image', 1),\n",
       " ('shrieking', 1),\n",
       " ('spitting', 1),\n",
       " ('year', 1),\n",
       " ('muttering', 1),\n",
       " ('pair', 1),\n",
       " ('watched', 1),\n",
       " ('leave', 1),\n",
       " ('proceeded', 1),\n",
       " ('deserted', 1),\n",
       " ('admit', 1),\n",
       " ('stalked', 1),\n",
       " ('undoubtedly', 1),\n",
       " ('number', 1),\n",
       " ('darkly', 1),\n",
       " ('mentioned', 1),\n",
       " ('soon', 1),\n",
       " ('sinking', 1),\n",
       " ('feeling', 1),\n",
       " ('fancy', 1),\n",
       " ('feeble', 1),\n",
       " ('surprise', 1),\n",
       " ('caught', 1),\n",
       " ('bright', 1),\n",
       " ('trusted', 1),\n",
       " ('tell', 1),\n",
       " ('word', 1),\n",
       " ('tried', 1),\n",
       " ('make', 1),\n",
       " ('sort', 1),\n",
       " ('grinning', 1),\n",
       " ('gargoyle', 1),\n",
       " ('grunting', 1),\n",
       " ('snores', 1),\n",
       " ('afternoon', 1),\n",
       " ('darted', 1),\n",
       " ('getting', 1),\n",
       " ('simple', 1),\n",
       " ('sounding', 1),\n",
       " ('annoyed', 1),\n",
       " ('perfectly', 1),\n",
       " ('invisibility', 1),\n",
       " ('silly', 1),\n",
       " ('trying', 1),\n",
       " ('mate', 1),\n",
       " ('noticed', 1),\n",
       " ('sunny', 1),\n",
       " ('finished', 1),\n",
       " ('threw', 1),\n",
       " ('triumph', 1),\n",
       " ('grinned', 1),\n",
       " ('gestured', 1),\n",
       " ('wordlessly', 1),\n",
       " ('shot', 1),\n",
       " ('old', 1),\n",
       " ('potions', 1),\n",
       " ('jinxes', 1),\n",
       " ('merely', 1),\n",
       " ('hit', 1),\n",
       " ('witches', 1),\n",
       " ('tightly', 1),\n",
       " ('automatically', 1),\n",
       " ('thinking', 1),\n",
       " ('crying', 1),\n",
       " ('new', 1),\n",
       " ('ranks', 1),\n",
       " ('dead', 1),\n",
       " ('headmasters', 1),\n",
       " ('nose', 1),\n",
       " ('peaceful', 1),\n",
       " ('glancing', 1),\n",
       " ('odd', 1),\n",
       " ('movement', 1),\n",
       " ('fell', 1),\n",
       " ('seamus', 1),\n",
       " ('staircase', 1),\n",
       " ('gaze', 1),\n",
       " ('wandered', 1),\n",
       " ('contained', 1),\n",
       " ('thundering', 1),\n",
       " ('woke', 1),\n",
       " ('wand', 1),\n",
       " ('forgotten', 1),\n",
       " ('sound', 1),\n",
       " ('lifted', 1),\n",
       " ('free', 1),\n",
       " ('snide', 1),\n",
       " ('slid', 1),\n",
       " ('bring', 1),\n",
       " ('anymore', 1),\n",
       " ('beaded', 1),\n",
       " ('fastened', 1),\n",
       " ('bringing', 1),\n",
       " ('gaping', 1),\n",
       " ('mirror', 1),\n",
       " ('propped', 1),\n",
       " ('beneath', 1),\n",
       " ('girl', 1),\n",
       " ('barman', 1),\n",
       " ('feet', 1),\n",
       " ('table', 1),\n",
       " ('smiled', 1),\n",
       " ('grew', 1),\n",
       " ('shoulders', 1),\n",
       " ('filled', 1),\n",
       " ('mantelpiece', 1),\n",
       " ('smooth', 1),\n",
       " ('stone', 1),\n",
       " ('steps', 1),\n",
       " ('sped', 1),\n",
       " ('alongside', 1),\n",
       " ('dressing', 1),\n",
       " ('gown', 1),\n",
       " ('speech', 1),\n",
       " ('dissolved', 1),\n",
       " ('scene', 1),\n",
       " ('shifted', 1),\n",
       " ('desk', 1),\n",
       " ('voldemort', 1),\n",
       " ('forest', 1),\n",
       " ('curtly', 1),\n",
       " ('careful', 1),\n",
       " ('kindly', 1),\n",
       " ('ghostly', 1),\n",
       " ('striding', 1),\n",
       " ('missing', 1),\n",
       " ('frames', 1),\n",
       " ('largest', 1),\n",
       " ('directly', 1),\n",
       " ('tears', 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(item, value) for item, value in contexts['portrait'].items()], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the word 'ghost'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(item, value) for item, value in contexts['ghost'].items()], key=lambda x: x[1], reverse=True)\n",
    "contexts['ghost'].items()\n",
    "type(contexts['ghost'].items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The co-occurrence matrix of a very large corpus should give a meaningful summary of how a word is used in general. A single row of that matrix is already a __word vector__ of size $N$. However such vectors are extremely sparse, and for large corpora the size of $N$ will become unwieldy. We will follow along the paper in designing a neural algorithm that can compress the word vectors while retaining most of their informational content. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b>\n",
    "It is not uncommon these days for the source corpus to have a size of at least a few billion words. For practical reasons our corpus in this assignment contains only about a million words: we can expect our results to be reasonable but of course not as great as with a much larger corpus.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.0:  Sparsity and Stability\n",
    "\n",
    "Our matrix $\\mathbf{X}$ is very sparse; most of its elements are zero. Find what the ratio of non-zero elements is.\n",
    "\n",
    "_Hint_: The function `non_zero_ratio` should return a `float` rather than a `FloatTensor`. Remember `.item()`.\n",
    "\n",
    "We will soon need to perform division and find the logarithm of $\\mathbf{X}$. Neither of the two operations are well-defined for $0$.\n",
    "\n",
    "Change the matrix's datatype to a `torch.float` and add a small constant to it (e.g. $0.1$) to ensure numerical stability while maintaining sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06063996713520976\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def non_zero_ratio(sparse_matrix: torch.Tensor) -> float:\n",
    "    sparse_matrix = sparse_matrix.type(torch.float)\n",
    "    nonzero = np.count_nonzero(sparse_matrix)\n",
    "    size = np.ma.size(sparse_matrix)\n",
    "    ratio = nonzero / size\n",
    "    return(ratio)\n",
    "    \n",
    "print(non_zero_ratio(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.type(torch.float) + 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.1: From co-occurrence counts to probabilities\n",
    "From the paper: \n",
    "> Let the matrix of word-word co-occurrence counts be denoted by $X$, whose entries $X_{ij}$ tabulate the number of times word $j$ occurs in the context of word $i$.  Let $X_i$= $\\sum_{k} X_{ik}$ be the number of times any word appears in the context of word $i$. Finally, let $P_{ij} = P(j  | i) =  X_{ij}/X_i$ be the probability that word $j$ appear in the context of word $i$. \n",
    "\n",
    "Complete the function `to_probabilities`, that accepts a co-occurrence matrix and returns the probability matrix $P$. \n",
    "\n",
    "_Hint_: Remember broadcasting and `torch.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4558e-04, 1.4558e-04, 1.4558e-04,  ..., 1.4558e-04, 1.4558e-04,\n",
       "         1.4558e-04],\n",
       "        [6.5236e-05, 6.5236e-05, 6.5236e-05,  ..., 6.5236e-05, 6.5236e-05,\n",
       "         6.5236e-05],\n",
       "        [1.4047e-04, 1.4047e-04, 1.4047e-04,  ..., 1.4047e-04, 1.4047e-04,\n",
       "         1.4047e-04],\n",
       "        ...,\n",
       "        [1.2872e-04, 1.2872e-04, 1.2872e-04,  ..., 1.2872e-04, 1.2872e-04,\n",
       "         1.2872e-04],\n",
       "        [1.1836e-04, 1.1836e-04, 1.1836e-04,  ..., 1.1836e-04, 1.1836e-04,\n",
       "         1.1836e-04],\n",
       "        [1.3247e-04, 1.3247e-04, 1.3247e-04,  ..., 1.3247e-04, 1.3247e-04,\n",
       "         1.3247e-04]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_probabilities(count_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    P = count_matrix\n",
    "    for i in range(0, len(P)):\n",
    "        P[i] = torch.div(P[i], torch.sum(P[i]))\n",
    "    return(P)\n",
    "\n",
    "P = to_probabilities(X)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.2: Probing words\n",
    "\n",
    "From the paper:\n",
    "> Consider two words $i$ and $j$ that exhibit a particular aspect of interest. The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, $k$.  For words $k$ related to $i$ but not $j$, we expect the ratio $P_{ik}/P_{jk}$ will be large.  Similarly, for words $k$ related to $j$ but not $i$, the ratio should be small. For words $k$ that are either related to both $i$ and $j$, or to neither, the ratio should be close to one.\n",
    "\n",
    "Complete the function `query` that accepts two words $w_i$ and $w_j$, a vocab $V$ and a probability matrix $\\mathbf{P}$, maps each word to its corresponding index and returns the probability $P(j  |  i)$.\n",
    "\n",
    "Then, complete the function `probe` that accepts three words $w_i$, $w_j$ and $w_k$, a vocab $V$ and a probability matrix $\\mathbf{P}$, calls `query` and returns the ratio $P(k |  i) / P(k  |  j)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0052)\n",
      "tensor(9.4293)\n",
      "tensor(0.1061)\n"
     ]
    }
   ],
   "source": [
    "def query(word_i: str, word_j: str, vocab: Dict[str, int], probability_matrix: torch.FloatTensor) -> float:  \n",
    "    i = vocab[word_i]\n",
    "    j = vocab[word_j]\n",
    "    P_ij = torch.div(P[i, j], torch.sum(P[i]))\n",
    "    return(P_ij)\n",
    "\n",
    "print(query(\"harry\", \"potter\", vocab, P))\n",
    "\n",
    "def probe(word_i: str, word_j: str, word_k: str, vocab: Dict[str, int], probability_matrix: torch.FloatTensor) -> float:\n",
    "    rt_PikPjk = query(word_i, word_k, vocab, probability_matrix) / query(word_j, word_k, vocab, probability_matrix)\n",
    "    return(rt_PikPjk)\n",
    "\n",
    "print(probe(\"harry\", \"stone\", \"potter\", vocab, P))\n",
    "print(probe(\"stone\", \"harry\", \"potter\", vocab, P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's probe a few words and examine whether the authors' claim holds even for our (tiny) corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3351)\n",
      "tensor(37.9336)\n",
      "tensor(8.3571)\n",
      "tensor(0.8429)\n",
      "tensor(0.0071)\n",
      "tensor(283.3288)\n",
      "tensor(6.8985)\n",
      "tensor(0.0027)\n",
      "tensor(0.5447)\n",
      "tensor(2.4607)\n"
     ]
    }
   ],
   "source": [
    "print(probe('tea', 'wand', 'spell', vocab, P))\n",
    "print(probe('tea', 'wand', 'cup', vocab, P))\n",
    "\n",
    "print(probe('voldemort', 'hagrid', 'curse', vocab, P))\n",
    "print(probe('voldemort', 'hagrid', 'beast', vocab, P))\n",
    "\n",
    "print(probe('mcgonagall', 'snape', 'potions', vocab, P))\n",
    "print(probe('mcgonagall', 'snape', 'transfiguration', vocab, P))\n",
    "\n",
    "print(probe('hedwig', 'scabbers', 'owl', vocab, P))\n",
    "print(probe('hedwig', 'scabbers', 'rat', vocab, P))\n",
    "\n",
    "print(probe('ron', 'hermione', 'book', vocab, P))\n",
    "print(probe('ron', 'hermione', 'red', vocab, P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it does seem like we are getting sensible results for in-domain words. Of course, probing out-of-domain words (such as the thermodynamics example the authors present) does not go all that well.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9290)\n",
      "tensor(0.9290)\n"
     ]
    }
   ],
   "source": [
    "print(probe('ice', 'steam', 'solid', vocab, P))\n",
    "print(probe('ice', 'steam', 'gas', vocab, P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Take home message: HP books are probably not the best textbook on thermodynamics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Vectors\n",
    "\n",
    "Now, we would like to convert these long, spare vectors into short, dense ones. \n",
    "\n",
    "The conversion should be such that the probability ratios we inspected earlier may still be reconstructed via some (for now, unknown) operation $F$ on the dense vectors.\n",
    "\n",
    "To restrict the search space over potential functions, the authors impose a number of constraints they think $F$ should satisfy:\n",
    "1. > While $F$ could be taken to be a complicated function parameterized by, e.g., a neural network, doing so would obfuscate the linear structure we are trying to capture. $F$ should be dot-product based.\n",
    "2. > The distinction between a word and a context word is arbitrary and we are free to exchange the two roles. To do so consistently, we must not only exchange $w \\leftrightarrow \\tilde{w}$ but also $X \\leftrightarrow X^T$.\n",
    "3. > It should be well-defined for all values in $X$.\n",
    "\n",
    "Given these three constraints, each word $i$ of our vocabulary is represented by four vectors:\n",
    "1. A vector $w_i \\in \\mathbb{R}^D$\n",
    "2. A bias $b_i \\in \\mathbb{R}$\n",
    "3. A context vector $\\tilde{w}_i \\in \\mathbb{R}^D$\n",
    "4. A context bias $\\tilde{b}_i \\in \\mathbb{R}$\n",
    "\n",
    "and $F: \\mathbb{R}^D \\times \\mathbb{R} \\times \\mathbb{R}^D \\times \\mathbb{R} \\to \\mathbb{R}$ is defined as:\n",
    "\n",
    "$F(w_i, \\tilde{w}_j, b_i, \\tilde{b}_k) = w_i^T\\tilde{w}_k + b_i + \\tilde{b}_k$\n",
    "\n",
    "such that $F(w_i, \\tilde{w}_k, b_i, \\tilde{b}_k)$ approximates $log(\\mathbf{X}_{ik})$, \n",
    "\n",
    "or equivallently the least squares error $J$ is minimized, where:\n",
    "\n",
    "$J = \\sum_{i,j=1}^{V} f(X_{ij})(w_{i}^T\\tilde{w}_j + b_i + \\tilde{b}_j - log(X_{ij}))^2$ the loss term\n",
    "\n",
    "and \n",
    "\n",
    "$f: \\mathbb{R} \\to \\mathbb{R} = \\begin{cases}\n",
    "    (x/x_{max})^a, & \\text{if $x<x_{max}$}\\\\\n",
    "    1, & \\text{otherwise}.\n",
    "  \\end{cases}$ a term weighting function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.3: Weighting Function\n",
    "\n",
    "Let's start with the last part. Complete the weighting function `weight_fn` which accepts a co-occurrence matrix $\\mathbf{X}$, a maximum value $x_{max}$ and a fractional power $a$, and returns the weighted co-occurrence matrix $f(\\mathbf{X})$.\n",
    "\n",
    "Then, compute `X_weighted`, the weighting of $\\mathbf{X}$, for $x_{max} = 100$ and $ a = \\frac{3}{4}$\n",
    "\n",
    "_Hint_: Note that $f$ is defined point-wise, so our weighting function should also be point-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0421)\n",
      "tensor(0.0029)\n",
      "tensor(0.0029)\n",
      "tensor([[4.1911e-05, 4.1911e-05, 4.1911e-05,  ..., 4.1911e-05, 4.1911e-05,\n",
      "         4.1911e-05],\n",
      "        [2.2954e-05, 2.2954e-05, 2.2954e-05,  ..., 2.2954e-05, 2.2954e-05,\n",
      "         2.2954e-05],\n",
      "        [4.0802e-05, 4.0802e-05, 4.0802e-05,  ..., 4.0802e-05, 4.0802e-05,\n",
      "         4.0802e-05],\n",
      "        ...,\n",
      "        [3.8214e-05, 3.8214e-05, 3.8214e-05,  ..., 3.8214e-05, 3.8214e-05,\n",
      "         3.8214e-05],\n",
      "        [3.5884e-05, 3.5884e-05, 3.5884e-05,  ..., 3.5884e-05, 3.5884e-05,\n",
      "         3.5884e-05],\n",
      "        [3.9047e-05, 3.9047e-05, 3.9047e-05,  ..., 3.9047e-05, 3.9047e-05,\n",
      "         3.9047e-05]])\n"
     ]
    }
   ],
   "source": [
    "def weight_fn(X: torch.FloatTensor, x_max: int, a: float) -> torch.FloatTensor:\n",
    "    X_weighted = torch.ones(X.shape)\n",
    "    X_weighted[X<x_max] = (X[X<x_max]/x_max)**a\n",
    "    return(X_weighted)\n",
    "\n",
    "print(X[vocab[\"harry\"],vocab[\"said\"]])\n",
    "X_weighted = weight_fn(X, x_max=100, a=3/4)\n",
    "print(X_weighted[vocab[\"harry\"],vocab[\"said\"]])\n",
    "print((X[vocab[\"harry\"],vocab[\"said\"]]/100)**(3/4))\n",
    "print(X_weighted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.4: Loss Function\n",
    "\n",
    "Next step is to write the loss function. \n",
    "\n",
    "We can write it as a point-wise function, apply it iteratively over each pair of words and then sum the result; that's however extremely inefficient. \n",
    "\n",
    "Inspecting the formulation of $J$, it is fairly straight-forward to see that it can be immediately implemented using matrix-matrix operations, as:\n",
    "\n",
    "$J = \\sum_{i,j=1}^{V}f(\\mathbf{X})(W\\tilde{W}^T + b + \\tilde{b} - log(X))^2$,\n",
    "\n",
    "where $W$, $\\tilde{W}$ the $N \\times D$ matrices containing the $D$-dimensional vectors of all our $N$ vocabulary words, and $b$, $\\tilde{b}$ the $N \\times 1$ matrices containing the $1$-dimensional biases of our words.\n",
    "\n",
    "Complete `loss_fn`, a function that accepts a weighted co-occurrence matrix $f(\\mathbf{X})$, the word vectors and biases $W$, $\\tilde{W}$, $b$, $\\tilde{b}$ and the co-occurrence matrix $\\mathbf{X}$, and computes $J$.\n",
    "\n",
    "Make sure that your completed `loss_fn` passes the `shape_test` function before you move on; if it does not, your math is wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(X_weighted: torch.FloatTensor, W: torch.FloatTensor, W_context: torch.FloatTensor, \n",
    "            B: torch.FloatTensor, B_context: torch.FloatTensor, \n",
    "            X: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    return(torch.sum(X_weighted*(W@W_context.t() + B + B_context.t() - X.log())**2))\n",
    "\n",
    "\n",
    "six_float_tensors = [torch.FloatTensor] * 6\n",
    "def shape_test(loss_fn: Callable[six_float_tensors, torch.FloatTensor]) -> bool:\n",
    "    n = 100\n",
    "    d = 12\n",
    "    rand_xw = torch.rand(n, n)\n",
    "    rand_w = torch.rand(n, d)\n",
    "    rand_wc = torch.rand(n, d)\n",
    "    rand_b = torch.rand(n, 1)\n",
    "    rand_bc = torch.rand(n, 1)\n",
    "    rand_x = torch.rand(n, n)\n",
    "    try:\n",
    "        loss = loss_fn(rand_xw, rand_w, rand_wc, rand_b, rand_bc, rand_x)\n",
    "        if loss.shape == torch.Size([]):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except IndexError:\n",
    "        return False\n",
    "shape_test(loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.5: GloVe\n",
    "\n",
    "We have the normalized co-occurrence matrix $\\mathbf{X}$, the weighting function $f$, and the loss function $J$ that implements $F$.\n",
    "\n",
    "What we need now is a mapping from words (or word ids) to unique, parametric and trainable vectors. \n",
    "\n",
    "Torch provides this abstraction in the form of [Embedding layers](https://pytorch.org/docs/stable/nn.html#embedding). Each such layer may be viewed as a stand-alone network, that can be optimized using the standard procedure we have already seen. \n",
    "\n",
    "We will instead contain them into a larger network that will be responsible for a few things:\n",
    "1. It wraps the embedding layers:\n",
    "    1. A vector embedding that maps words to $w \\in \\mathbb{R}^D$\n",
    "    1. A context vector embedding that maps words to $w_c \\in \\mathbb{R}^D$\n",
    "    1. A bias embedding that maps words to $b \\in \\mathbb{R}^1$\n",
    "    1. A context bias embedding that maps words to $b_c \\in \\mathbb{R}^1$\n",
    "1. It implements `forward`, a function that accepts a weighted co-occurrence matrix $f(\\mathbf{X})$, the co-occurrence matrix $\\mathbf{X}$, then computes the embeddings of all words and finally calls `loss_fn` as defined above.\n",
    "1. It implements `get_vectors`, a function that receives no input and produces the word vectors and context word vectors of all words, adds them together and returns the result, in accordance with the paper:\n",
    "> .. With this in mind, we choose to use the sum $W + \\tilde{W}$ as our word vectors.\n",
    "\n",
    "Complete the network class following the above specifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(torch.nn.Module):\n",
    "    def __init__(self, vocab: Dict[str, int], vector_dimensionality: int=2**5, device: str='cpu') -> None:\n",
    "        super(GloVe, self).__init__()\n",
    "        self.device = device\n",
    "        self.vocab_len = len(vocab)\n",
    "        self.w = torch.nn.Embedding(num_embeddings = self.vocab_len, embedding_dim=vector_dimensionality).to(self.device)\n",
    "        self.wc = torch.nn.Embedding(num_embeddings = self.vocab_len, embedding_dim=vector_dimensionality).to(self.device)\n",
    "        self.b = torch.nn.Embedding(num_embeddings = self.vocab_len, embedding_dim=1).to(self.device)\n",
    "        self.bc = torch.nn.Embedding(num_embeddings = self.vocab_len, embedding_dim=1).to(self.device)\n",
    "        \n",
    "    def forward(self, X_weighted: torch.FloatTensor, X: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        embedding_input = torch.arange(self.vocab_len).to(self.device)\n",
    "        W = self.w(embedding_input)\n",
    "        Wc = self.wc(embedding_input)\n",
    "        B = self.b(embedding_input)\n",
    "        Bc = self.bc(embedding_input)\n",
    "        return loss_fn(X_weighted, W, Wc, B, Bc, X)\n",
    "    \n",
    "    def get_vectors(self):\n",
    "        embedding_input = torch.arange(self.vocab_len).to(self.device)\n",
    "        W = self.w(embedding_input)\n",
    "        Wc = self.wc(embedding_input)\n",
    "        return W + Wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.6: Training\n",
    "\n",
    "Everything is in place; now we may begin optimizing our embedding layers (and in doing so, the vectors they assign). Instantiate the network class you just defined, using $D = 30$. Then instantiate an `Adam` optimizer with a learning rate of 0.05 and train your network for 300 epochs.\n",
    "\n",
    "When writing the training script, remember that your network's forward pass is __already__ computing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      " the loss is: 136526.828125.\n",
      "Epoch 1:\n",
      " the loss is: 120602.671875.\n",
      "Epoch 2:\n",
      " the loss is: 107692.1875.\n",
      "Epoch 3:\n",
      " the loss is: 96669.7109375.\n",
      "Epoch 4:\n",
      " the loss is: 86178.25.\n",
      "Epoch 5:\n",
      " the loss is: 74771.359375.\n",
      "Epoch 6:\n",
      " the loss is: 61336.296875.\n",
      "Epoch 7:\n",
      " the loss is: 45721.58203125.\n",
      "Epoch 8:\n",
      " the loss is: 29375.9453125.\n",
      "Epoch 9:\n",
      " the loss is: 15822.693359375.\n",
      "Epoch 10:\n",
      " the loss is: 10015.353515625.\n",
      "Epoch 11:\n",
      " the loss is: 13261.33984375.\n",
      "Epoch 12:\n",
      " the loss is: 18225.220703125.\n",
      "Epoch 13:\n",
      " the loss is: 18130.078125.\n",
      "Epoch 14:\n",
      " the loss is: 13529.3798828125.\n",
      "Epoch 15:\n",
      " the loss is: 8245.189453125.\n",
      "Epoch 16:\n",
      " the loss is: 5051.66845703125.\n",
      "Epoch 17:\n",
      " the loss is: 4461.63671875.\n",
      "Epoch 18:\n",
      " the loss is: 5518.19091796875.\n",
      "Epoch 19:\n",
      " the loss is: 6985.9736328125.\n",
      "Epoch 20:\n",
      " the loss is: 7998.22265625.\n",
      "Epoch 21:\n",
      " the loss is: 8156.9638671875.\n",
      "Epoch 22:\n",
      " the loss is: 7420.3857421875.\n",
      "Epoch 23:\n",
      " the loss is: 5993.55419921875.\n",
      "Epoch 24:\n",
      " the loss is: 4265.6337890625.\n",
      "Epoch 25:\n",
      " the loss is: 2748.54150390625.\n",
      "Epoch 26:\n",
      " the loss is: 1947.03125.\n",
      "Epoch 27:\n",
      " the loss is: 2109.39501953125.\n",
      "Epoch 28:\n",
      " the loss is: 2953.4228515625.\n",
      "Epoch 29:\n",
      " the loss is: 3745.21923828125.\n",
      "Epoch 30:\n",
      " the loss is: 3872.86962890625.\n",
      "Epoch 31:\n",
      " the loss is: 3295.564208984375.\n",
      "Epoch 32:\n",
      " the loss is: 2437.463134765625.\n",
      "Epoch 33:\n",
      " the loss is: 1778.3143310546875.\n",
      "Epoch 34:\n",
      " the loss is: 1548.734619140625.\n",
      "Epoch 35:\n",
      " the loss is: 1690.5050048828125.\n",
      "Epoch 36:\n",
      " the loss is: 1991.05810546875.\n",
      "Epoch 37:\n",
      " the loss is: 2236.14892578125.\n",
      "Epoch 38:\n",
      " the loss is: 2297.16259765625.\n",
      "Epoch 39:\n",
      " the loss is: 2152.47705078125.\n",
      "Epoch 40:\n",
      " the loss is: 1869.969482421875.\n",
      "Epoch 41:\n",
      " the loss is: 1568.5308837890625.\n",
      "Epoch 42:\n",
      " the loss is: 1366.74560546875.\n",
      "Epoch 43:\n",
      " the loss is: 1328.96142578125.\n",
      "Epoch 44:\n",
      " the loss is: 1431.4990234375.\n",
      "Epoch 45:\n",
      " the loss is: 1575.5869140625.\n",
      "Epoch 46:\n",
      " the loss is: 1649.83154296875.\n",
      "Epoch 47:\n",
      " the loss is: 1601.875732421875.\n",
      "Epoch 48:\n",
      " the loss is: 1464.012939453125.\n",
      "Epoch 49:\n",
      " the loss is: 1317.634765625.\n",
      "Epoch 50:\n",
      " the loss is: 1233.027099609375.\n",
      "Epoch 51:\n",
      " the loss is: 1231.0830078125.\n",
      "Epoch 52:\n",
      " the loss is: 1284.362548828125.\n",
      "Epoch 53:\n",
      " the loss is: 1343.8541259765625.\n",
      "Epoch 54:\n",
      " the loss is: 1368.74755859375.\n",
      "Epoch 55:\n",
      " the loss is: 1344.197265625.\n",
      "Epoch 56:\n",
      " the loss is: 1283.28271484375.\n",
      "Epoch 57:\n",
      " the loss is: 1216.1488037109375.\n",
      "Epoch 58:\n",
      " the loss is: 1172.557861328125.\n",
      "Epoch 59:\n",
      " the loss is: 1166.01123046875.\n",
      "Epoch 60:\n",
      " the loss is: 1187.565185546875.\n",
      "Epoch 61:\n",
      " the loss is: 1213.19384765625.\n",
      "Epoch 62:\n",
      " the loss is: 1220.250732421875.\n",
      "Epoch 63:\n",
      " the loss is: 1201.63232421875.\n",
      "Epoch 64:\n",
      " the loss is: 1167.945556640625.\n",
      "Epoch 65:\n",
      " the loss is: 1137.5372314453125.\n",
      "Epoch 66:\n",
      " the loss is: 1123.383056640625.\n",
      "Epoch 67:\n",
      " the loss is: 1126.128173828125.\n",
      "Epoch 68:\n",
      " the loss is: 1136.2962646484375.\n",
      "Epoch 69:\n",
      " the loss is: 1141.9901123046875.\n",
      "Epoch 70:\n",
      " the loss is: 1136.33447265625.\n",
      "Epoch 71:\n",
      " the loss is: 1120.55029296875.\n",
      "Epoch 72:\n",
      " the loss is: 1101.876953125.\n",
      "Epoch 73:\n",
      " the loss is: 1088.33935546875.\n",
      "Epoch 74:\n",
      " the loss is: 1083.69140625.\n",
      "Epoch 75:\n",
      " the loss is: 1085.596923828125.\n",
      "Epoch 76:\n",
      " the loss is: 1087.9580078125.\n",
      "Epoch 77:\n",
      " the loss is: 1085.482421875.\n",
      "Epoch 78:\n",
      " the loss is: 1077.128662109375.\n",
      "Epoch 79:\n",
      " the loss is: 1066.1446533203125.\n",
      "Epoch 80:\n",
      " the loss is: 1057.1346435546875.\n",
      "Epoch 81:\n",
      " the loss is: 1052.7093505859375.\n",
      "Epoch 82:\n",
      " the loss is: 1052.105712890625.\n",
      "Epoch 83:\n",
      " the loss is: 1052.341552734375.\n",
      "Epoch 84:\n",
      " the loss is: 1050.5736083984375.\n",
      "Epoch 85:\n",
      " the loss is: 1045.958251953125.\n",
      "Epoch 86:\n",
      " the loss is: 1039.83544921875.\n",
      "Epoch 87:\n",
      " the loss is: 1034.483154296875.\n",
      "Epoch 88:\n",
      " the loss is: 1031.3941650390625.\n",
      "Epoch 89:\n",
      " the loss is: 1030.36181640625.\n",
      "Epoch 90:\n",
      " the loss is: 1029.87451171875.\n",
      "Epoch 91:\n",
      " the loss is: 1028.376220703125.\n",
      "Epoch 92:\n",
      " the loss is: 1025.3775634765625.\n",
      "Epoch 93:\n",
      " the loss is: 1021.6298217773438.\n",
      "Epoch 94:\n",
      " the loss is: 1018.3375244140625.\n",
      "Epoch 95:\n",
      " the loss is: 1016.1873779296875.\n",
      "Epoch 96:\n",
      " the loss is: 1014.9490966796875.\n",
      "Epoch 97:\n",
      " the loss is: 1013.7921142578125.\n",
      "Epoch 98:\n",
      " the loss is: 1011.9864501953125.\n",
      "Epoch 99:\n",
      " the loss is: 1009.4256591796875.\n",
      "Epoch 100:\n",
      " the loss is: 1006.5778198242188.\n",
      "Epoch 101:\n",
      " the loss is: 1004.0660400390625.\n",
      "Epoch 102:\n",
      " the loss is: 1002.150390625.\n",
      "Epoch 103:\n",
      " the loss is: 1000.6260986328125.\n",
      "Epoch 104:\n",
      " the loss is: 999.0379638671875.\n",
      "Epoch 105:\n",
      " the loss is: 997.0745849609375.\n",
      "Epoch 106:\n",
      " the loss is: 994.787353515625.\n",
      "Epoch 107:\n",
      " the loss is: 992.4814453125.\n",
      "Epoch 108:\n",
      " the loss is: 990.4395141601562.\n",
      "Epoch 109:\n",
      " the loss is: 988.6996459960938.\n",
      "Epoch 110:\n",
      " the loss is: 987.0665283203125.\n",
      "Epoch 111:\n",
      " the loss is: 985.30517578125.\n",
      "Epoch 112:\n",
      " the loss is: 983.3316650390625.\n",
      "Epoch 113:\n",
      " the loss is: 981.24365234375.\n",
      "Epoch 114:\n",
      " the loss is: 979.2218017578125.\n",
      "Epoch 115:\n",
      " the loss is: 977.3753051757812.\n",
      "Epoch 116:\n",
      " the loss is: 975.6654052734375.\n",
      "Epoch 117:\n",
      " the loss is: 973.9613037109375.\n",
      "Epoch 118:\n",
      " the loss is: 972.1724853515625.\n",
      "Epoch 119:\n",
      " the loss is: 970.3126220703125.\n",
      "Epoch 120:\n",
      " the loss is: 968.4671630859375.\n",
      "Epoch 121:\n",
      " the loss is: 966.7203369140625.\n",
      "Epoch 122:\n",
      " the loss is: 965.0819091796875.\n",
      "Epoch 123:\n",
      " the loss is: 963.4920043945312.\n",
      "Epoch 124:\n",
      " the loss is: 961.885009765625.\n",
      "Epoch 125:\n",
      " the loss is: 960.242919921875.\n",
      "Epoch 126:\n",
      " the loss is: 958.6049194335938.\n",
      "Epoch 127:\n",
      " the loss is: 957.0195922851562.\n",
      "Epoch 128:\n",
      " the loss is: 955.5081787109375.\n",
      "Epoch 129:\n",
      " the loss is: 954.0479736328125.\n",
      "Epoch 130:\n",
      " the loss is: 952.6025390625.\n",
      "Epoch 131:\n",
      " the loss is: 951.1492919921875.\n",
      "Epoch 132:\n",
      " the loss is: 949.69873046875.\n",
      "Epoch 133:\n",
      " the loss is: 948.2774658203125.\n",
      "Epoch 134:\n",
      " the loss is: 946.9034423828125.\n",
      "Epoch 135:\n",
      " the loss is: 945.5614013671875.\n",
      "Epoch 136:\n",
      " the loss is: 944.2379150390625.\n",
      "Epoch 137:\n",
      " the loss is: 942.90966796875.\n",
      "Epoch 138:\n",
      " the loss is: 941.58642578125.\n",
      "Epoch 139:\n",
      " the loss is: 940.27978515625.\n",
      "Epoch 140:\n",
      " the loss is: 939.0057373046875.\n",
      "Epoch 141:\n",
      " the loss is: 937.758544921875.\n",
      "Epoch 142:\n",
      " the loss is: 936.530029296875.\n",
      "Epoch 143:\n",
      " the loss is: 935.3094482421875.\n",
      "Epoch 144:\n",
      " the loss is: 934.099609375.\n",
      "Epoch 145:\n",
      " the loss is: 932.9058837890625.\n",
      "Epoch 146:\n",
      " the loss is: 931.7374877929688.\n",
      "Epoch 147:\n",
      " the loss is: 930.590087890625.\n",
      "Epoch 148:\n",
      " the loss is: 929.4595947265625.\n",
      "Epoch 149:\n",
      " the loss is: 928.33984375.\n",
      "Epoch 150:\n",
      " the loss is: 927.2305908203125.\n",
      "Epoch 151:\n",
      " the loss is: 926.13623046875.\n",
      "Epoch 152:\n",
      " the loss is: 925.060791015625.\n",
      "Epoch 153:\n",
      " the loss is: 924.0047607421875.\n",
      "Epoch 154:\n",
      " the loss is: 922.9625244140625.\n",
      "Epoch 155:\n",
      " the loss is: 921.935546875.\n",
      "Epoch 156:\n",
      " the loss is: 920.92041015625.\n",
      "Epoch 157:\n",
      " the loss is: 919.9185791015625.\n",
      "Epoch 158:\n",
      " the loss is: 918.9339599609375.\n",
      "Epoch 159:\n",
      " the loss is: 917.962158203125.\n",
      "Epoch 160:\n",
      " the loss is: 917.0048828125.\n",
      "Epoch 161:\n",
      " the loss is: 916.0579833984375.\n",
      "Epoch 162:\n",
      " the loss is: 915.122314453125.\n",
      "Epoch 163:\n",
      " the loss is: 914.1966552734375.\n",
      "Epoch 164:\n",
      " the loss is: 913.284423828125.\n",
      "Epoch 165:\n",
      " the loss is: 912.3853759765625.\n",
      "Epoch 166:\n",
      " the loss is: 911.4954833984375.\n",
      "Epoch 167:\n",
      " the loss is: 910.616455078125.\n",
      "Epoch 168:\n",
      " the loss is: 909.7489013671875.\n",
      "Epoch 169:\n",
      " the loss is: 908.8914184570312.\n",
      "Epoch 170:\n",
      " the loss is: 908.0445556640625.\n",
      "Epoch 171:\n",
      " the loss is: 907.20556640625.\n",
      "Epoch 172:\n",
      " the loss is: 906.376953125.\n",
      "Epoch 173:\n",
      " the loss is: 905.5562744140625.\n",
      "Epoch 174:\n",
      " the loss is: 904.7420654296875.\n",
      "Epoch 175:\n",
      " the loss is: 903.9381103515625.\n",
      "Epoch 176:\n",
      " the loss is: 903.142822265625.\n",
      "Epoch 177:\n",
      " the loss is: 902.353759765625.\n",
      "Epoch 178:\n",
      " the loss is: 901.571533203125.\n",
      "Epoch 179:\n",
      " the loss is: 900.7991943359375.\n",
      "Epoch 180:\n",
      " the loss is: 900.0318603515625.\n",
      "Epoch 181:\n",
      " the loss is: 899.2718505859375.\n",
      "Epoch 182:\n",
      " the loss is: 898.51904296875.\n",
      "Epoch 183:\n",
      " the loss is: 897.7727661132812.\n",
      "Epoch 184:\n",
      " the loss is: 897.0321044921875.\n",
      "Epoch 185:\n",
      " the loss is: 896.298095703125.\n",
      "Epoch 186:\n",
      " the loss is: 895.569091796875.\n",
      "Epoch 187:\n",
      " the loss is: 894.8463134765625.\n",
      "Epoch 188:\n",
      " the loss is: 894.1279907226562.\n",
      "Epoch 189:\n",
      " the loss is: 893.4156494140625.\n",
      "Epoch 190:\n",
      " the loss is: 892.7093505859375.\n",
      "Epoch 191:\n",
      " the loss is: 892.0064086914062.\n",
      "Epoch 192:\n",
      " the loss is: 891.311279296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193:\n",
      " the loss is: 890.6185913085938.\n",
      "Epoch 194:\n",
      " the loss is: 889.9315185546875.\n",
      "Epoch 195:\n",
      " the loss is: 889.247802734375.\n",
      "Epoch 196:\n",
      " the loss is: 888.5709228515625.\n",
      "Epoch 197:\n",
      " the loss is: 887.8970947265625.\n",
      "Epoch 198:\n",
      " the loss is: 887.2276000976562.\n",
      "Epoch 199:\n",
      " the loss is: 886.5621337890625.\n",
      "Epoch 200:\n",
      " the loss is: 885.900634765625.\n",
      "Epoch 201:\n",
      " the loss is: 885.2442016601562.\n",
      "Epoch 202:\n",
      " the loss is: 884.5926513671875.\n",
      "Epoch 203:\n",
      " the loss is: 883.9449462890625.\n",
      "Epoch 204:\n",
      " the loss is: 883.301025390625.\n",
      "Epoch 205:\n",
      " the loss is: 882.6603393554688.\n",
      "Epoch 206:\n",
      " the loss is: 882.024658203125.\n",
      "Epoch 207:\n",
      " the loss is: 881.3917846679688.\n",
      "Epoch 208:\n",
      " the loss is: 880.7620849609375.\n",
      "Epoch 209:\n",
      " the loss is: 880.1376342773438.\n",
      "Epoch 210:\n",
      " the loss is: 879.51806640625.\n",
      "Epoch 211:\n",
      " the loss is: 878.900390625.\n",
      "Epoch 212:\n",
      " the loss is: 878.288330078125.\n",
      "Epoch 213:\n",
      " the loss is: 877.678955078125.\n",
      "Epoch 214:\n",
      " the loss is: 877.074951171875.\n",
      "Epoch 215:\n",
      " the loss is: 876.4732666015625.\n",
      "Epoch 216:\n",
      " the loss is: 875.876708984375.\n",
      "Epoch 217:\n",
      " the loss is: 875.2830810546875.\n",
      "Epoch 218:\n",
      " the loss is: 874.6932373046875.\n",
      "Epoch 219:\n",
      " the loss is: 874.107421875.\n",
      "Epoch 220:\n",
      " the loss is: 873.5252685546875.\n",
      "Epoch 221:\n",
      " the loss is: 872.948486328125.\n",
      "Epoch 222:\n",
      " the loss is: 872.3740844726562.\n",
      "Epoch 223:\n",
      " the loss is: 871.8037109375.\n",
      "Epoch 224:\n",
      " the loss is: 871.2372436523438.\n",
      "Epoch 225:\n",
      " the loss is: 870.675537109375.\n",
      "Epoch 226:\n",
      " the loss is: 870.1165161132812.\n",
      "Epoch 227:\n",
      " the loss is: 869.563232421875.\n",
      "Epoch 228:\n",
      " the loss is: 869.0147705078125.\n",
      "Epoch 229:\n",
      " the loss is: 868.4681396484375.\n",
      "Epoch 230:\n",
      " the loss is: 867.9266357421875.\n",
      "Epoch 231:\n",
      " the loss is: 867.3880615234375.\n",
      "Epoch 232:\n",
      " the loss is: 866.854248046875.\n",
      "Epoch 233:\n",
      " the loss is: 866.32421875.\n",
      "Epoch 234:\n",
      " the loss is: 865.7974853515625.\n",
      "Epoch 235:\n",
      " the loss is: 865.274658203125.\n",
      "Epoch 236:\n",
      " the loss is: 864.75634765625.\n",
      "Epoch 237:\n",
      " the loss is: 864.2412109375.\n",
      "Epoch 238:\n",
      " the loss is: 863.731689453125.\n",
      "Epoch 239:\n",
      " the loss is: 863.2264404296875.\n",
      "Epoch 240:\n",
      " the loss is: 862.7215576171875.\n",
      "Epoch 241:\n",
      " the loss is: 862.2227172851562.\n",
      "Epoch 242:\n",
      " the loss is: 861.72802734375.\n",
      "Epoch 243:\n",
      " the loss is: 861.2381591796875.\n",
      "Epoch 244:\n",
      " the loss is: 860.7520751953125.\n",
      "Epoch 245:\n",
      " the loss is: 860.2699584960938.\n",
      "Epoch 246:\n",
      " the loss is: 859.78955078125.\n",
      "Epoch 247:\n",
      " the loss is: 859.3153076171875.\n",
      "Epoch 248:\n",
      " the loss is: 858.843994140625.\n",
      "Epoch 249:\n",
      " the loss is: 858.376708984375.\n",
      "Epoch 250:\n",
      " the loss is: 857.91357421875.\n",
      "Epoch 251:\n",
      " the loss is: 857.45556640625.\n",
      "Epoch 252:\n",
      " the loss is: 856.9990234375.\n",
      "Epoch 253:\n",
      " the loss is: 856.545654296875.\n",
      "Epoch 254:\n",
      " the loss is: 856.0970458984375.\n",
      "Epoch 255:\n",
      " the loss is: 855.65283203125.\n",
      "Epoch 256:\n",
      " the loss is: 855.2118530273438.\n",
      "Epoch 257:\n",
      " the loss is: 854.774658203125.\n",
      "Epoch 258:\n",
      " the loss is: 854.34033203125.\n",
      "Epoch 259:\n",
      " the loss is: 853.91064453125.\n",
      "Epoch 260:\n",
      " the loss is: 853.4839477539062.\n",
      "Epoch 261:\n",
      " the loss is: 853.0614013671875.\n",
      "Epoch 262:\n",
      " the loss is: 852.6409912109375.\n",
      "Epoch 263:\n",
      " the loss is: 852.22509765625.\n",
      "Epoch 264:\n",
      " the loss is: 851.8135986328125.\n",
      "Epoch 265:\n",
      " the loss is: 851.404052734375.\n",
      "Epoch 266:\n",
      " the loss is: 850.998046875.\n",
      "Epoch 267:\n",
      " the loss is: 850.5948486328125.\n",
      "Epoch 268:\n",
      " the loss is: 850.19580078125.\n",
      "Epoch 269:\n",
      " the loss is: 849.8016357421875.\n",
      "Epoch 270:\n",
      " the loss is: 849.4088134765625.\n",
      "Epoch 271:\n",
      " the loss is: 849.0195922851562.\n",
      "Epoch 272:\n",
      " the loss is: 848.6337890625.\n",
      "Epoch 273:\n",
      " the loss is: 848.2518920898438.\n",
      "Epoch 274:\n",
      " the loss is: 847.871826171875.\n",
      "Epoch 275:\n",
      " the loss is: 847.4957275390625.\n",
      "Epoch 276:\n",
      " the loss is: 847.122314453125.\n",
      "Epoch 277:\n",
      " the loss is: 846.7518920898438.\n",
      "Epoch 278:\n",
      " the loss is: 846.3836669921875.\n",
      "Epoch 279:\n",
      " the loss is: 846.0201416015625.\n",
      "Epoch 280:\n",
      " the loss is: 845.6575927734375.\n",
      "Epoch 281:\n",
      " the loss is: 845.3001098632812.\n",
      "Epoch 282:\n",
      " the loss is: 844.9453735351562.\n",
      "Epoch 283:\n",
      " the loss is: 844.5925903320312.\n",
      "Epoch 284:\n",
      " the loss is: 844.2412109375.\n",
      "Epoch 285:\n",
      " the loss is: 843.89404296875.\n",
      "Epoch 286:\n",
      " the loss is: 843.5504150390625.\n",
      "Epoch 287:\n",
      " the loss is: 843.2088623046875.\n",
      "Epoch 288:\n",
      " the loss is: 842.868896484375.\n",
      "Epoch 289:\n",
      " the loss is: 842.5321044921875.\n",
      "Epoch 290:\n",
      " the loss is: 842.1982421875.\n",
      "Epoch 291:\n",
      " the loss is: 841.8680419921875.\n",
      "Epoch 292:\n",
      " the loss is: 841.5391845703125.\n",
      "Epoch 293:\n",
      " the loss is: 841.21240234375.\n",
      "Epoch 294:\n",
      " the loss is: 840.8892211914062.\n",
      "Epoch 295:\n",
      " the loss is: 840.568115234375.\n",
      "Epoch 296:\n",
      " the loss is: 840.2500610351562.\n",
      "Epoch 297:\n",
      " the loss is: 839.9337158203125.\n",
      "Epoch 298:\n",
      " the loss is: 839.6209106445312.\n",
      "Epoch 299:\n",
      " the loss is: 839.309326171875.\n",
      "Epoch 300:\n",
      " the loss is: 839.0.\n",
      "Epoch 301:\n",
      " the loss is: 838.6934814453125.\n",
      "Epoch 302:\n",
      " the loss is: 838.388671875.\n",
      "Epoch 303:\n",
      " the loss is: 838.085693359375.\n",
      "Epoch 304:\n",
      " the loss is: 837.784423828125.\n",
      "Epoch 305:\n",
      " the loss is: 837.4864501953125.\n",
      "Epoch 306:\n",
      " the loss is: 837.1912841796875.\n",
      "Epoch 307:\n",
      " the loss is: 836.8974609375.\n",
      "Epoch 308:\n",
      " the loss is: 836.607177734375.\n",
      "Epoch 309:\n",
      " the loss is: 836.3175048828125.\n",
      "Epoch 310:\n",
      " the loss is: 836.0303955078125.\n",
      "Epoch 311:\n",
      " the loss is: 835.7456665039062.\n",
      "Epoch 312:\n",
      " the loss is: 835.462646484375.\n",
      "Epoch 313:\n",
      " the loss is: 835.1812744140625.\n",
      "Epoch 314:\n",
      " the loss is: 834.9039306640625.\n",
      "Epoch 315:\n",
      " the loss is: 834.6260986328125.\n",
      "Epoch 316:\n",
      " the loss is: 834.351318359375.\n",
      "Epoch 317:\n",
      " the loss is: 834.0782470703125.\n",
      "Epoch 318:\n",
      " the loss is: 833.80712890625.\n",
      "Epoch 319:\n",
      " the loss is: 833.5380859375.\n",
      "Epoch 320:\n",
      " the loss is: 833.271484375.\n",
      "Epoch 321:\n",
      " the loss is: 833.0057373046875.\n",
      "Epoch 322:\n",
      " the loss is: 832.7420654296875.\n",
      "Epoch 323:\n",
      " the loss is: 832.4805908203125.\n",
      "Epoch 324:\n",
      " the loss is: 832.2212524414062.\n",
      "Epoch 325:\n",
      " the loss is: 831.9642333984375.\n",
      "Epoch 326:\n",
      " the loss is: 831.7069702148438.\n",
      "Epoch 327:\n",
      " the loss is: 831.4522705078125.\n",
      "Epoch 328:\n",
      " the loss is: 831.2005004882812.\n",
      "Epoch 329:\n",
      " the loss is: 830.9497680664062.\n",
      "Epoch 330:\n",
      " the loss is: 830.70166015625.\n",
      "Epoch 331:\n",
      " the loss is: 830.4524536132812.\n",
      "Epoch 332:\n",
      " the loss is: 830.2066650390625.\n",
      "Epoch 333:\n",
      " the loss is: 829.96435546875.\n",
      "Epoch 334:\n",
      " the loss is: 829.7216796875.\n",
      "Epoch 335:\n",
      " the loss is: 829.480224609375.\n",
      "Epoch 336:\n",
      " the loss is: 829.240966796875.\n",
      "Epoch 337:\n",
      " the loss is: 829.0029296875.\n",
      "Epoch 338:\n",
      " the loss is: 828.76806640625.\n",
      "Epoch 339:\n",
      " the loss is: 828.5335083007812.\n",
      "Epoch 340:\n",
      " the loss is: 828.3006591796875.\n",
      "Epoch 341:\n",
      " the loss is: 828.069580078125.\n",
      "Epoch 342:\n",
      " the loss is: 827.8404541015625.\n",
      "Epoch 343:\n",
      " the loss is: 827.6129150390625.\n",
      "Epoch 344:\n",
      " the loss is: 827.38720703125.\n",
      "Epoch 345:\n",
      " the loss is: 827.1624755859375.\n",
      "Epoch 346:\n",
      " the loss is: 826.9390869140625.\n",
      "Epoch 347:\n",
      " the loss is: 826.7181396484375.\n",
      "Epoch 348:\n",
      " the loss is: 826.4967041015625.\n",
      "Epoch 349:\n",
      " the loss is: 826.279296875.\n",
      "Epoch 350:\n",
      " the loss is: 826.0618896484375.\n",
      "Epoch 351:\n",
      " the loss is: 825.845947265625.\n",
      "Epoch 352:\n",
      " the loss is: 825.632080078125.\n",
      "Epoch 353:\n",
      " the loss is: 825.4189453125.\n",
      "Epoch 354:\n",
      " the loss is: 825.2061767578125.\n",
      "Epoch 355:\n",
      " the loss is: 824.995849609375.\n",
      "Epoch 356:\n",
      " the loss is: 824.787841796875.\n",
      "Epoch 357:\n",
      " the loss is: 824.5799560546875.\n",
      "Epoch 358:\n",
      " the loss is: 824.3734130859375.\n",
      "Epoch 359:\n",
      " the loss is: 824.1697387695312.\n",
      "Epoch 360:\n",
      " the loss is: 823.966552734375.\n",
      "Epoch 361:\n",
      " the loss is: 823.7647094726562.\n",
      "Epoch 362:\n",
      " the loss is: 823.56396484375.\n",
      "Epoch 363:\n",
      " the loss is: 823.3665771484375.\n",
      "Epoch 364:\n",
      " the loss is: 823.1683349609375.\n",
      "Epoch 365:\n",
      " the loss is: 822.9705810546875.\n",
      "Epoch 366:\n",
      " the loss is: 822.7757568359375.\n",
      "Epoch 367:\n",
      " the loss is: 822.5821533203125.\n",
      "Epoch 368:\n",
      " the loss is: 822.3890380859375.\n",
      "Epoch 369:\n",
      " the loss is: 822.1986083984375.\n",
      "Epoch 370:\n",
      " the loss is: 822.0084228515625.\n",
      "Epoch 371:\n",
      " the loss is: 821.819580078125.\n",
      "Epoch 372:\n",
      " the loss is: 821.63232421875.\n",
      "Epoch 373:\n",
      " the loss is: 821.4453125.\n",
      "Epoch 374:\n",
      " the loss is: 821.2587890625.\n",
      "Epoch 375:\n",
      " the loss is: 821.07421875.\n",
      "Epoch 376:\n",
      " the loss is: 820.8919677734375.\n",
      "Epoch 377:\n",
      " the loss is: 820.710693359375.\n",
      "Epoch 378:\n",
      " the loss is: 820.5299072265625.\n",
      "Epoch 379:\n",
      " the loss is: 820.3504638671875.\n",
      "Epoch 380:\n",
      " the loss is: 820.17236328125.\n",
      "Epoch 381:\n",
      " the loss is: 819.9962158203125.\n",
      "Epoch 382:\n",
      " the loss is: 819.8214111328125.\n",
      "Epoch 383:\n",
      " the loss is: 819.646484375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384:\n",
      " the loss is: 819.4730224609375.\n",
      "Epoch 385:\n",
      " the loss is: 819.30078125.\n",
      "Epoch 386:\n",
      " the loss is: 819.1297607421875.\n",
      "Epoch 387:\n",
      " the loss is: 818.9589233398438.\n",
      "Epoch 388:\n",
      " the loss is: 818.7899780273438.\n",
      "Epoch 389:\n",
      " the loss is: 818.6236572265625.\n",
      "Epoch 390:\n",
      " the loss is: 818.4573974609375.\n",
      "Epoch 391:\n",
      " the loss is: 818.2921752929688.\n",
      "Epoch 392:\n",
      " the loss is: 818.1282958984375.\n",
      "Epoch 393:\n",
      " the loss is: 817.96435546875.\n",
      "Epoch 394:\n",
      " the loss is: 817.8018798828125.\n",
      "Epoch 395:\n",
      " the loss is: 817.6409301757812.\n",
      "Epoch 396:\n",
      " the loss is: 817.4808349609375.\n",
      "Epoch 397:\n",
      " the loss is: 817.321533203125.\n",
      "Epoch 398:\n",
      " the loss is: 817.1637573242188.\n",
      "Epoch 399:\n",
      " the loss is: 817.0067138671875.\n",
      "Epoch 400:\n",
      " the loss is: 816.8516845703125.\n",
      "Epoch 401:\n",
      " the loss is: 816.6962280273438.\n",
      "Epoch 402:\n",
      " the loss is: 816.5423583984375.\n",
      "Epoch 403:\n",
      " the loss is: 816.3890380859375.\n",
      "Epoch 404:\n",
      " the loss is: 816.237548828125.\n",
      "Epoch 405:\n",
      " the loss is: 816.0858154296875.\n",
      "Epoch 406:\n",
      " the loss is: 815.9365234375.\n",
      "Epoch 407:\n",
      " the loss is: 815.7877197265625.\n",
      "Epoch 408:\n",
      " the loss is: 815.6397094726562.\n",
      "Epoch 409:\n",
      " the loss is: 815.4920654296875.\n",
      "Epoch 410:\n",
      " the loss is: 815.3461303710938.\n",
      "Epoch 411:\n",
      " the loss is: 815.2011108398438.\n",
      "Epoch 412:\n",
      " the loss is: 815.057373046875.\n",
      "Epoch 413:\n",
      " the loss is: 814.913818359375.\n",
      "Epoch 414:\n",
      " the loss is: 814.7722778320312.\n",
      "Epoch 415:\n",
      " the loss is: 814.6309814453125.\n",
      "Epoch 416:\n",
      " the loss is: 814.4898681640625.\n",
      "Epoch 417:\n",
      " the loss is: 814.351318359375.\n",
      "Epoch 418:\n",
      " the loss is: 814.2122802734375.\n",
      "Epoch 419:\n",
      " the loss is: 814.0748901367188.\n",
      "Epoch 420:\n",
      " the loss is: 813.93896484375.\n",
      "Epoch 421:\n",
      " the loss is: 813.8031005859375.\n",
      "Epoch 422:\n",
      " the loss is: 813.6680297851562.\n",
      "Epoch 423:\n",
      " the loss is: 813.5338134765625.\n",
      "Epoch 424:\n",
      " the loss is: 813.3997802734375.\n",
      "Epoch 425:\n",
      " the loss is: 813.2669677734375.\n",
      "Epoch 426:\n",
      " the loss is: 813.1358642578125.\n",
      "Epoch 427:\n",
      " the loss is: 813.00537109375.\n",
      "Epoch 428:\n",
      " the loss is: 812.87548828125.\n",
      "Epoch 429:\n",
      " the loss is: 812.747802734375.\n",
      "Epoch 430:\n",
      " the loss is: 812.619384765625.\n",
      "Epoch 431:\n",
      " the loss is: 812.4918212890625.\n",
      "Epoch 432:\n",
      " the loss is: 812.3658447265625.\n",
      "Epoch 433:\n",
      " the loss is: 812.2407836914062.\n",
      "Epoch 434:\n",
      " the loss is: 812.1146240234375.\n",
      "Epoch 435:\n",
      " the loss is: 811.9912719726562.\n",
      "Epoch 436:\n",
      " the loss is: 811.868896484375.\n",
      "Epoch 437:\n",
      " the loss is: 811.7464599609375.\n",
      "Epoch 438:\n",
      " the loss is: 811.624755859375.\n",
      "Epoch 439:\n",
      " the loss is: 811.5042114257812.\n",
      "Epoch 440:\n",
      " the loss is: 811.3826904296875.\n",
      "Epoch 441:\n",
      " the loss is: 811.263671875.\n",
      "Epoch 442:\n",
      " the loss is: 811.1448974609375.\n",
      "Epoch 443:\n",
      " the loss is: 811.0277709960938.\n",
      "Epoch 444:\n",
      " the loss is: 810.9110107421875.\n",
      "Epoch 445:\n",
      " the loss is: 810.794189453125.\n",
      "Epoch 446:\n",
      " the loss is: 810.6787109375.\n",
      "Epoch 447:\n",
      " the loss is: 810.5650634765625.\n",
      "Epoch 448:\n",
      " the loss is: 810.4512329101562.\n",
      "Epoch 449:\n",
      " the loss is: 810.337890625.\n",
      "Epoch 450:\n",
      " the loss is: 810.2252807617188.\n",
      "Epoch 451:\n",
      " the loss is: 810.1133422851562.\n",
      "Epoch 452:\n",
      " the loss is: 810.002685546875.\n",
      "Epoch 453:\n",
      " the loss is: 809.8916625976562.\n",
      "Epoch 454:\n",
      " the loss is: 809.7820434570312.\n",
      "Epoch 455:\n",
      " the loss is: 809.673583984375.\n",
      "Epoch 456:\n",
      " the loss is: 809.5648193359375.\n",
      "Epoch 457:\n",
      " the loss is: 809.4573974609375.\n",
      "Epoch 458:\n",
      " the loss is: 809.350830078125.\n",
      "Epoch 459:\n",
      " the loss is: 809.24462890625.\n",
      "Epoch 460:\n",
      " the loss is: 809.1396484375.\n",
      "Epoch 461:\n",
      " the loss is: 809.0344848632812.\n",
      "Epoch 462:\n",
      " the loss is: 808.93115234375.\n",
      "Epoch 463:\n",
      " the loss is: 808.8279418945312.\n",
      "Epoch 464:\n",
      " the loss is: 808.7252197265625.\n",
      "Epoch 465:\n",
      " the loss is: 808.62255859375.\n",
      "Epoch 466:\n",
      " the loss is: 808.521728515625.\n",
      "Epoch 467:\n",
      " the loss is: 808.4202880859375.\n",
      "Epoch 468:\n",
      " the loss is: 808.3197631835938.\n",
      "Epoch 469:\n",
      " the loss is: 808.219970703125.\n",
      "Epoch 470:\n",
      " the loss is: 808.1210327148438.\n",
      "Epoch 471:\n",
      " the loss is: 808.023193359375.\n",
      "Epoch 472:\n",
      " the loss is: 807.9264526367188.\n",
      "Epoch 473:\n",
      " the loss is: 807.8292236328125.\n",
      "Epoch 474:\n",
      " the loss is: 807.7333984375.\n",
      "Epoch 475:\n",
      " the loss is: 807.638427734375.\n",
      "Epoch 476:\n",
      " the loss is: 807.54345703125.\n",
      "Epoch 477:\n",
      " the loss is: 807.448974609375.\n",
      "Epoch 478:\n",
      " the loss is: 807.355712890625.\n",
      "Epoch 479:\n",
      " the loss is: 807.2627563476562.\n",
      "Epoch 480:\n",
      " the loss is: 807.1700439453125.\n",
      "Epoch 481:\n",
      " the loss is: 807.0779418945312.\n",
      "Epoch 482:\n",
      " the loss is: 806.9859619140625.\n",
      "Epoch 483:\n",
      " the loss is: 806.8963623046875.\n",
      "Epoch 484:\n",
      " the loss is: 806.8059692382812.\n",
      "Epoch 485:\n",
      " the loss is: 806.7161865234375.\n",
      "Epoch 486:\n",
      " the loss is: 806.627685546875.\n",
      "Epoch 487:\n",
      " the loss is: 806.538818359375.\n",
      "Epoch 488:\n",
      " the loss is: 806.4508666992188.\n",
      "Epoch 489:\n",
      " the loss is: 806.3633422851562.\n",
      "Epoch 490:\n",
      " the loss is: 806.2772216796875.\n",
      "Epoch 491:\n",
      " the loss is: 806.1904296875.\n",
      "Epoch 492:\n",
      " the loss is: 806.1044921875.\n",
      "Epoch 493:\n",
      " the loss is: 806.0193481445312.\n",
      "Epoch 494:\n",
      " the loss is: 805.93505859375.\n",
      "Epoch 495:\n",
      " the loss is: 805.8516845703125.\n",
      "Epoch 496:\n",
      " the loss is: 805.7679443359375.\n",
      "Epoch 497:\n",
      " the loss is: 805.6849365234375.\n",
      "Epoch 498:\n",
      " the loss is: 805.6024169921875.\n",
      "Epoch 499:\n",
      " the loss is: 805.520263671875.\n",
      "Epoch 500:\n",
      " the loss is: 805.4387817382812.\n",
      "Epoch 501:\n",
      " the loss is: 805.3570556640625.\n",
      "Epoch 502:\n",
      " the loss is: 805.2772827148438.\n",
      "Epoch 503:\n",
      " the loss is: 805.196533203125.\n",
      "Epoch 504:\n",
      " the loss is: 805.1163940429688.\n",
      "Epoch 505:\n",
      " the loss is: 805.0374755859375.\n",
      "Epoch 506:\n",
      " the loss is: 804.9595947265625.\n",
      "Epoch 507:\n",
      " the loss is: 804.8817749023438.\n",
      "Epoch 508:\n",
      " the loss is: 804.8040771484375.\n",
      "Epoch 509:\n",
      " the loss is: 804.7269287109375.\n",
      "Epoch 510:\n",
      " the loss is: 804.6502685546875.\n",
      "Epoch 511:\n",
      " the loss is: 804.5743408203125.\n",
      "Epoch 512:\n",
      " the loss is: 804.499267578125.\n",
      "Epoch 513:\n",
      " the loss is: 804.4244384765625.\n",
      "Epoch 514:\n",
      " the loss is: 804.35009765625.\n",
      "Epoch 515:\n",
      " the loss is: 804.276123046875.\n",
      "Epoch 516:\n",
      " the loss is: 804.2017822265625.\n",
      "Epoch 517:\n",
      " the loss is: 804.1288452148438.\n",
      "Epoch 518:\n",
      " the loss is: 804.0551147460938.\n",
      "Epoch 519:\n",
      " the loss is: 803.9833374023438.\n",
      "Epoch 520:\n",
      " the loss is: 803.910888671875.\n",
      "Epoch 521:\n",
      " the loss is: 803.8394775390625.\n",
      "Epoch 522:\n",
      " the loss is: 803.7684936523438.\n",
      "Epoch 523:\n",
      " the loss is: 803.6981811523438.\n",
      "Epoch 524:\n",
      " the loss is: 803.6279296875.\n",
      "Epoch 525:\n",
      " the loss is: 803.5584716796875.\n",
      "Epoch 526:\n",
      " the loss is: 803.4895629882812.\n",
      "Epoch 527:\n",
      " the loss is: 803.4202880859375.\n",
      "Epoch 528:\n",
      " the loss is: 803.3514404296875.\n",
      "Epoch 529:\n",
      " the loss is: 803.283935546875.\n",
      "Epoch 530:\n",
      " the loss is: 803.2161865234375.\n",
      "Epoch 531:\n",
      " the loss is: 803.149658203125.\n",
      "Epoch 532:\n",
      " the loss is: 803.0821533203125.\n",
      "Epoch 533:\n",
      " the loss is: 803.01708984375.\n",
      "Epoch 534:\n",
      " the loss is: 802.9512939453125.\n",
      "Epoch 535:\n",
      " the loss is: 802.885009765625.\n",
      "Epoch 536:\n",
      " the loss is: 802.8197021484375.\n",
      "Epoch 537:\n",
      " the loss is: 802.7554321289062.\n",
      "Epoch 538:\n",
      " the loss is: 802.691162109375.\n",
      "Epoch 539:\n",
      " the loss is: 802.6277465820312.\n",
      "Epoch 540:\n",
      " the loss is: 802.5634765625.\n",
      "Epoch 541:\n",
      " the loss is: 802.4993896484375.\n",
      "Epoch 542:\n",
      " the loss is: 802.4378662109375.\n",
      "Epoch 543:\n",
      " the loss is: 802.3758544921875.\n",
      "Epoch 544:\n",
      " the loss is: 802.314453125.\n",
      "Epoch 545:\n",
      " the loss is: 802.251953125.\n",
      "Epoch 546:\n",
      " the loss is: 802.19140625.\n",
      "Epoch 547:\n",
      " the loss is: 802.131103515625.\n",
      "Epoch 548:\n",
      " the loss is: 802.0700073242188.\n",
      "Epoch 549:\n",
      " the loss is: 802.0105590820312.\n",
      "Epoch 550:\n",
      " the loss is: 801.950927734375.\n",
      "Epoch 551:\n",
      " the loss is: 801.8914794921875.\n",
      "Epoch 552:\n",
      " the loss is: 801.832763671875.\n",
      "Epoch 553:\n",
      " the loss is: 801.773681640625.\n",
      "Epoch 554:\n",
      " the loss is: 801.7152099609375.\n",
      "Epoch 555:\n",
      " the loss is: 801.6573486328125.\n",
      "Epoch 556:\n",
      " the loss is: 801.599365234375.\n",
      "Epoch 557:\n",
      " the loss is: 801.5432739257812.\n",
      "Epoch 558:\n",
      " the loss is: 801.4857177734375.\n",
      "Epoch 559:\n",
      " the loss is: 801.4293212890625.\n",
      "Epoch 560:\n",
      " the loss is: 801.3729248046875.\n",
      "Epoch 561:\n",
      " the loss is: 801.3167114257812.\n",
      "Epoch 562:\n",
      " the loss is: 801.2606201171875.\n",
      "Epoch 563:\n",
      " the loss is: 801.2056884765625.\n",
      "Epoch 564:\n",
      " the loss is: 801.1506958007812.\n",
      "Epoch 565:\n",
      " the loss is: 801.0969848632812.\n",
      "Epoch 566:\n",
      " the loss is: 801.0435791015625.\n",
      "Epoch 567:\n",
      " the loss is: 800.9901733398438.\n",
      "Epoch 568:\n",
      " the loss is: 800.936767578125.\n",
      "Epoch 569:\n",
      " the loss is: 800.8834228515625.\n",
      "Epoch 570:\n",
      " the loss is: 800.8308715820312.\n",
      "Epoch 571:\n",
      " the loss is: 800.7781372070312.\n",
      "Epoch 572:\n",
      " the loss is: 800.7261962890625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 573:\n",
      " the loss is: 800.6744995117188.\n",
      "Epoch 574:\n",
      " the loss is: 800.6234130859375.\n",
      "Epoch 575:\n",
      " the loss is: 800.5718994140625.\n",
      "Epoch 576:\n",
      " the loss is: 800.5203857421875.\n",
      "Epoch 577:\n",
      " the loss is: 800.469482421875.\n",
      "Epoch 578:\n",
      " the loss is: 800.418212890625.\n",
      "Epoch 579:\n",
      " the loss is: 800.367919921875.\n",
      "Epoch 580:\n",
      " the loss is: 800.3180541992188.\n",
      "Epoch 581:\n",
      " the loss is: 800.26806640625.\n",
      "Epoch 582:\n",
      " the loss is: 800.2191162109375.\n",
      "Epoch 583:\n",
      " the loss is: 800.1708984375.\n",
      "Epoch 584:\n",
      " the loss is: 800.12255859375.\n",
      "Epoch 585:\n",
      " the loss is: 800.0736083984375.\n",
      "Epoch 586:\n",
      " the loss is: 800.0272216796875.\n",
      "Epoch 587:\n",
      " the loss is: 799.9800415039062.\n",
      "Epoch 588:\n",
      " the loss is: 799.9320678710938.\n",
      "Epoch 589:\n",
      " the loss is: 799.8843994140625.\n",
      "Epoch 590:\n",
      " the loss is: 799.836669921875.\n",
      "Epoch 591:\n",
      " the loss is: 799.7901611328125.\n",
      "Epoch 592:\n",
      " the loss is: 799.7433471679688.\n",
      "Epoch 593:\n",
      " the loss is: 799.6970825195312.\n",
      "Epoch 594:\n",
      " the loss is: 799.6513671875.\n",
      "Epoch 595:\n",
      " the loss is: 799.605224609375.\n",
      "Epoch 596:\n",
      " the loss is: 799.56005859375.\n",
      "Epoch 597:\n",
      " the loss is: 799.5152587890625.\n",
      "Epoch 598:\n",
      " the loss is: 799.4708251953125.\n",
      "Epoch 599:\n",
      " the loss is: 799.42626953125.\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "network = GloVe(vocab = vocab)\n",
    "opt = torch.optim.Adam(network.parameters(), lr = 0.1)\n",
    "\n",
    "num_epochs = 600\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss = network.forward(X_weighted = X_weighted, X = X) # compute loss (optionally print)\n",
    "    print(f\"Epoch {i}:\\n the loss is: {loss}.\")\n",
    "    loss.backward() # backward\n",
    "    opt.step() # optimizer step\n",
    "    opt.zero_grad() # zero grads\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.7: Validation (Similarity)\n",
    "\n",
    "Curious to see what this network has learned? Let's perform a simple validation experiment. \n",
    "\n",
    "We will check which words the models considers the most similar to other words. To that end, we need a notion of __similarity__. One of the most common measures of similarity in high dimensional vector spaces is the cosine similarity. \n",
    "\n",
    "The cosine similarity of two vectors $\\vec{a}, \\vec{b}$ is given as:\n",
    "$$sim(\\vec{a}, \\vec{b}) = \\frac{\\vec{a}\\cdot \\vec{b}}{|\\vec{a}| \\cdot |\\vec{b}|}$$\n",
    "\n",
    "where $|\\vec{x}|$ the length of $\\vec{x}$.\n",
    "\n",
    "The function `similarity` below accepts two words, a vocabulary and the network's output vectors, and computes the similarity between these two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word_i: str, word_j: str, vocab: Dict[str, int], vectors: torch.FloatTensor) -> float:\n",
    "    i = vocab[word_i]\n",
    "    j = vocab[word_j] \n",
    "    v_i = vectors[i] / torch.norm(vectors[i], p=2)  # a/|a|\n",
    "    v_j = vectors[j] / torch.norm(vectors[j], p=2)  # b/|b|\n",
    "    sim = torch.mm(v_i.view(1, -1), v_j.view(-1, 1)).item()\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some examples (try your own word pairs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between cruciatus and imperius is: 0.7491680383682251\n",
      "Similarity between avada and kedavra is: 0.9821314215660095\n",
      "Similarity between hogwarts and school is: 0.9200867414474487\n",
      "Similarity between evil and hagrid is: -0.06140298768877983\n",
      "Similarity between good and hagrid is: 0.5283257961273193\n",
      "Similarity between thomas and kedavra is: 0.013822022825479507\n",
      "Similarity between harry and potter is: 0.4975874423980713\n"
     ]
    }
   ],
   "source": [
    "word_vectors = network.get_vectors().detach()\n",
    "\n",
    "for pair in [('cruciatus', 'imperius'), \n",
    "             ('avada', 'kedavra'), \n",
    "             ('hogwarts', 'school'), \n",
    "             ('evil', 'hagrid'), \n",
    "             ('good', 'hagrid'),\n",
    "            (\"thomas\", \"kedavra\"),\n",
    "            (\"harry\", \"potter\")]:\n",
    "    \n",
    "    print('Similarity between {} and {} is: {}'.\n",
    "          format(pair[0], pair[1], similarity(pair[0], pair[1], vocab, word_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the similarities of one word against all other words in the corpus, we may rewrite the above equation as:\n",
    "$$sim(\\vec{w}, \\mathbf{C}) = \\frac{\\vec{w}\\cdot \\mathbf{C}}{|\\vec{w}| \\cdot |\\mathbf{C}|}$$\n",
    "\n",
    "Using `similarity` as a reference, write `similarities`, which accepts one word, a vocabulary and the network's output vectors and computes the similarity between the word and the entire corpus.\n",
    "\n",
    "_Hint_: $\\mathbf{C} \\in \\mathbb{R}^{N, D}$, $\\vec{w} \\in \\mathbb{R}^{1, D}$, $sim(\\vec{w}, \\mathbf{C}) \\in \\mathbb{R}^{1, N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarities(word_i: str, vocab: Dict[str, int], vectors: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    i = vocab[word_i]\n",
    "    word_i = word_vectors[i] / torch.norm(word_vectors[i], p=2)  # w/|w|\n",
    "    C = word_vectors / torch.norm(word_vectors, p=2)  # C/|C|\n",
    "    sim = torch.mm(word_i.view(1, -1), C.t())\n",
    "    return sim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can manipulate the word vectors to find out what the corpus-wide most similar words to a query word is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word_i: str, vocab: Dict[str, int], vectors: torch.FloatTensor, k: int) -> List[str]:\n",
    "    sims = similarities(word_i, vocab, vectors)\n",
    "    _, topi = argmax_top_k(sims, k)\n",
    "    topi = topi.cpu().numpy().tolist()\n",
    "    inv = {v: i for i, v in vocab.items()}\n",
    "    return [inv[i[0]] for i in topi]\n",
    "    \n",
    "def argmax_top_k(x, k: int):\n",
    "    copy = x.clone().detach().requires_grad_(False)\n",
    "    retv, reti = [], []\n",
    "    for repeat in range(k):\n",
    "        values, indices = torch.max(copy, dim=-1)\n",
    "        mask = torch.arange(x.size(-1), device=x.device).reshape(1, -1) == indices.unsqueeze(-1)\n",
    "        copy[mask] = -float('inf')\n",
    "        retv.append(values)\n",
    "        reti.append(indices)\n",
    "    retv, reti = torch.stack(retv), torch.stack(reti)\n",
    "    return retv, reti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to forbidden: ['forest', 'forbidden', 'trees', 'thirty', 'grounds', 'lake']\n",
      "Most similar words to myrtle: ['myrtle', 'moaning', 'bathroom', 'diary', 'tears', 'kreacher']\n",
      "Most similar words to gryffindor: ['gryffindor', 'slytherin', 'ravenclaw', 'team', 'points', 'quidditch']\n",
      "Most similar words to wand: ['wand', 'raised', 'wands', 'spell', 'tip', 'ollivander']\n",
      "Most similar words to quidditch: ['quidditch', 'team', 'match', 'gryffindor', 'cup', 'points']\n",
      "Most similar words to marauder: ['map', 'marauder', 'invisibility', 'cloak', 'bag', 'pocket']\n",
      "Most similar words to horcrux: ['horcrux', 'locket', 'sword', 'hallows', 'soul', 'horcruxes']\n",
      "Most similar words to phoenix: ['phoenix', 'fawkes', 'order', 'feather', 'chamber', 'golden']\n",
      "Most similar words to triwizard: ['tournament', 'triwizard', 'cup', 'task', 'bagman', 'champions']\n",
      "Most similar words to screaming: ['screaming', 'laughter', 'scream', 'pain', 'screamed', 'louder']\n",
      "Most similar words to letter: ['letter', 'owl', 'prophet', 'parchment', 'envelope', 'read']\n",
      "Most similar words to snape: ['snape', 'professor', 'severus', 'dark', 'malfoy', 'mcgonagall']\n",
      "Most similar words to dumbledore: ['dumbledore', 'fudge', 'voldemort', 'lord', 'albus', 'minister']\n",
      "Most similar words to astronomy: ['tower', 'astronomy', 'clock', 'defense', 'classroom', 'arts']\n"
     ]
    }
   ],
   "source": [
    "for word in ['forbidden', 'myrtle', 'gryffindor', 'wand', 'quidditch', 'marauder', 'horcrux', 'phoenix', 'triwizard', 'screaming',\n",
    "            'letter', 'snape', 'dumbledore', 'astronomy'\n",
    "            ]:\n",
    "    print('Most similar words to {}: {}'.format(word, most_similar(word, vocab, word_vectors, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite impressive; we managed to encode a meaningful portion of the corpus statistics in such $30$ numbers in each word! \n",
    "(A compression ratio of 99.4%)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> The word vectors obtained by this process are (to a small extent) random, due to the random initialization of the embedding layers. If you are unhappy with your results, you can repeat the experiment a few times or try to toy around with the hyper-parameters (the smoothing factor of $\\mathbf{X}$, $x_{max}$, $a$, the number of epochs and the dimensionality of the vector space).\n",
    "</div>\n",
    "\n",
    "Word vectors, however, can contain way more information than just word co-occurrence statistics. Hold tight until the next assignment, where we will see how word vectors may be used to infer information spanning entire phrases and sentences.\n",
    "\n",
    "If you feel like probing the results some more, continue on with the bonus assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.8: Shortcomings\n",
    "Evidently, GloVe offers a simple and computationally efficient means to construct dense word representations.\n",
    "However, the means of vectorization suffers from a few important shortcomings.\n",
    "Can you imagine what these are? Write a few sentences each on at least two of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here. \n",
    "\n",
    "***There is the problem related to Zipf's law: there are more instances of words that occur rarely, than the sum of the frequencies of the most common words. The relatedness of words that do not occur often is difficult to compute accurately, since there are not many context examples to rely on.***\n",
    "\n",
    "***Furthermore, vectorisation is very limited to the domain of the corpus. As we saw earlier, such a model does not perform as well on judging the relatedness of words that are considered to be closely related in other domains than in that of the corpus.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1.9 (BONUS part): Validation (Word Analogies)\n",
    "\n",
    "From the paper:\n",
    "> The word analogy task consists of questions like \"_a_ is to _b_ as is _c_ to ?\" To correctly answer this question, we must find the word d such that $w_d \\approx w_b - w_a + w_c$ according to the cosine similarity.\n",
    "\n",
    "Write your own function that performs the word analogy task.\n",
    "\n",
    "_Hint_: Take a look at the code a few cells back. Most of what you need is already there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(word_a: str, word_b: str, word_c: str, vocab: Dict[str, int], vectors: torch.FloatTensor, k: int) -> str:\n",
    "    sim_ab = similarity(word_a, word_b, vocab, word_vectors)\n",
    "    analogies = []\n",
    "    for word_d in vocab:\n",
    "        sim_cd = similarity(word_c, word_d, vocab, word_vectors)\n",
    "        analogies.append([word_d, abs(sim_ab - sim_cd)])\n",
    "    analogies = sorted(analogies, key=lambda kv: kv[1])\n",
    "    analogies = analogies[0:k]\n",
    "    return([i[0] for i in analogies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example triplets to test your analogies on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padma is to parvati as fred is to ['twins', 'percy', 'ginny', 'weasley', 'ron', 'george']\n",
      "avada is to kedavra as expecto is to ['patronum', 'expecto', 'stag', 'blinding', 'erupted', 'shudder']\n",
      "dungeon is to slytherin as tower is to ['hardly', 'shared', 'robes', 'pay', 'ragged', 'britain']\n",
      "scabbers is to ron as hedwig is to ['asleep', 'flown', 'carried', 'crookshanks', 'screech', 'letter']\n",
      "ron is to molly as draco is to ['minerva', 'wiping', 'snapped', 'mrs', 'pomfrey', 'parents']\n",
      "durmstrang is to viktor as beauxbatons is to ['party', 'delacour', 'champions', 'enclosure', 'students', 'arriving']\n",
      "snape is to potions as trelawney is to ['flitwick', 'blankly', 'herbology', 'closely', 'star', 'plank']\n",
      "harry is to seeker as ron is to ['fletchley', 'sniggering', 'umbrella', 'beaters', 'line', 'favor']\n"
     ]
    }
   ],
   "source": [
    "triplets = [('padma', 'parvati', 'fred'),\n",
    "            ('avada', 'kedavra', 'expecto'),\n",
    "            ('dungeon', 'slytherin', 'tower'),\n",
    "            ('scabbers', 'ron', 'hedwig'),\n",
    "            ('ron', 'molly', 'draco'),\n",
    "            ('durmstrang', 'viktor', 'beauxbatons'),\n",
    "            ('snape', 'potions', 'trelawney'),\n",
    "            ('harry', 'seeker', 'ron')\n",
    "           ]\n",
    "\n",
    "for a, b, c in triplets:\n",
    "    print('{} is to {} as {} is to {}'.format(a, b, c, analogy(a, b, c, vocab, word_vectors, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some minimal emergent intelligence :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧙‍♀️"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
