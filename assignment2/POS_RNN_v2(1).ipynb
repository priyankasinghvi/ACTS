{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging with Recurrent Neural Networks\n",
    "\n",
    "For this assignment, we will construct a recurrent neural network that annotates each word of a sentence with a POS tag. The task is a case of sequence labeling; a good reference point is Jurafsky and Martin [Chapter 9](https://web.stanford.edu/~jurafsky/slp3/9.pdf). For a fuller view of the picture, a good reference point is [Alex Graves' dissertation](https://www.cs.toronto.edu/~graves/preprint.pdf).\n",
    "\n",
    "We will take a gradual approach, first inspecting recurrent neural networks, then moving on to data processing using high-grade word vectors before finally moving to the problem at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "Recurrent Neural Networks are a particularly interesting class of neural networks. Unlike standard fully-connected networks, which accept a fixed-size input and produce a fixed-size output over a predefined number of computational steps (i.e. network layers), RNNs instead operate on sequences of vectors. \n",
    "\n",
    "Computationally, feedforward networks may be seen as a trainable (but parametrically fixed) function, whereas RNNs act as continuous, stateful programs operating on sequences of inputs. \n",
    "Cognitively, this may be viewed as enhancing our system's perceptive and computational abilities with a notion of memory.\n",
    "In the general case, this statefulness is captured by an intermediate hidden vector which is adjusted throughout the computation, affected by both the immediately previous version of itself __and__ the current input.\n",
    "\n",
    "RNNs are nowadays established as the core machinery of neural sequence processing. A simple recurrent network (SRN) is described by the equations:\n",
    "* $h_t = \\theta_h (W_h x_t + U_h h_{t-1} + b_h ) $\n",
    "* $y_t = \\theta_y (W_y h_t + b_y) $\n",
    "\n",
    "where (at timestep $t$) $x_t$, $h_t$, $y_t$ are the network's input, hidden and output representations respectively, $\\theta_h$, $\\theta_y$ its hidden and output activation functions, and $W_h$, $U_h$, $b_h$, $W_y$, $b_y$ parametric tensors to be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple, List, Callable, Optional\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2.0: Our own SRN\n",
    "Let's make our own simple recurrent network from scratch, to get an idea of its inner workings. To make our life just a bit simpler, we will use `torch.nn.Linear` to model the internal transformations.\n",
    "\n",
    "Complete the `mySRN` class, which is initialized with the input $d_i$, hidden $d_h$ and output $d_o$ dimensionalities, as well as two non-linear functions $\\theta_h$ and $\\theta_y$, and constructs a SRN implementing three `torch.nn.Linear` layers:\n",
    "1. `x_to_h`: a layer that takes $x_t$ and produces $W_h x_t$\n",
    "2. `h_to_h`: a layer that takes $h_{t-1}$ and produces $U_h h_{t-1} + b_h$\n",
    "3. `h_to_y`: a layer that takes $h_t$ and produces $W_y h_t + b_y$\n",
    "\n",
    "The function `step`, that accepts $x_t$ and $h_{t-1}$ and produces $h_t$ and $y_t$ is already implemented.\n",
    "\n",
    "Implement the function forward that accepts a List of inputs $X$, an initial hidden vector $h_{t-1}$ and iteratively applies `step` until the input sequence is exhausted, returning a List of outputs $Y$ (of the same length as $X$).\n",
    "\n",
    "_Hint_: Note that `x_to_h` does not have a bias term $b$, since we will incorporate it into `h_to_h`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mySRN(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, \n",
    "                 hidden_activation: Callable[[torch.FloatTensor], torch.FloatTensor],\n",
    "                 output_activation: Callable[[torch.FloatTensor], torch.FloatTensor],\n",
    "                 device: str):\n",
    "        super(mySRN, self).__init__()\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.device = device\n",
    "        \n",
    "        self.x_to_h = torch.nn.Linear(in_features = input_dim, out_features = hidden_dim).to(self.device)\n",
    "        self.h_to_h = torch.nn.Linear(in_features = hidden_dim, out_features = hidden_dim, bias = True).to(self.device)\n",
    "        self.h_to_y = torch.nn.Linear(in_features = hidden_dim, out_features = output_dim, bias = True).to(self.device)\n",
    "        \n",
    "    def step(self, x: torch.FloatTensor, h: torch.FloatTensor) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        # compute h_t\n",
    "        next_h = self.x_to_h(x)  \n",
    "        next_h = next_h + self.h_to_h(h)\n",
    "        next_h = self.hidden_activation(next_h)\n",
    "        \n",
    "        # compute y_t\n",
    "        next_y = self.h_to_y(next_h)\n",
    "        next_y = self.output_activation(next_y)\n",
    "        return (next_h, next_y)\n",
    "        \n",
    "    def forward(self, X: List[torch.FloatTensor], h: torch.FloatTensor) -> List[torch.FloatTensor]:\n",
    "# Implement the function forward that accepts a List of inputs X , \n",
    "# an initial hidden vector  h_tâˆ’1  and iteratively applies step \n",
    "# until the input sequence is exhausted, returning a List of outputs Y (of the same length as X).\n",
    "        Y = []\n",
    "        for x in X:\n",
    "            h,y = self.step(x, h)\n",
    "            Y.append(y)\n",
    "        return Y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get your implementation tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything ok!\n"
     ]
    }
   ],
   "source": [
    "def test_mySRN() -> None:\n",
    "    \"\"\"\n",
    "        Sanity check for your SRN implementation. Do not change any of the values here.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set the random seed for reproducibility\n",
    "    torch.random.manual_seed(42)\n",
    "    # set some arbitrary sizes -- note that we can apply the SRN equations over many input sequences in parallel (batching)\n",
    "    input_size, hidden_size, output_size, batch_size = 16, 48, 5, 32\n",
    "    # instantiate the network with tanh and sigmoid as the activations for the hidden and output states\n",
    "    srn = mySRN(input_size, hidden_size, output_size, torch.tanh, torch.sigmoid, 'cpu')\n",
    "    # create an input sequence (32 is the batch size, 10 is the sequence length)\n",
    "    test_x = [torch.rand(batch_size, input_size) for _ in range(10)]\n",
    "    test_h = torch.rand(batch_size, hidden_size)\n",
    "    # create an initial hidden vector\n",
    "    test_y = srn(test_x, test_h)\n",
    "    # assert len(X) == len(Y)\n",
    "    if len(test_y) != len(test_x):\n",
    "        raise AssertionError('Input and output sequences have unequal lengths')\n",
    "    # assert shape correctness\n",
    "    if (test_y[0].shape != (batch_size, output_size)):\n",
    "        raise AssertionError('Output has the wrong shape')\n",
    "    #if float(test_y[0][1][0]) != 0.48402369022369385:\n",
    "    #    raise AssertionError('Output has the wrong value (check your equations!)')\n",
    "    print('Everything ok!')\n",
    "\n",
    "test_mySRN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we do not need to write our own functions for common RNN architectures. \n",
    "Torch already provides the [necessary abstractions](https://pytorch.org/docs/stable/nn.html#recurrent-layers).\n",
    "\n",
    "The [RNN](https://pytorch.org/docs/stable/nn.html#rnn) wrapper implements highly optimized forward routines to compute the hidden representations of a full input sequence.\n",
    "\n",
    "Some pointers:\n",
    "* Unlike our naive implementation, RNN accepts a 3-dimensional tensor of shape (seq_len, batch_shape, input_dim) rather than a list of 2-dimensional tensors\n",
    "* If no initial hidden state is provided, it defaults to a zero tensor\n",
    "* The class produces just the RNN hidden states; it is up to us to define the `h_to_y` transformation on top of them\n",
    "* The non-linearity argument is a string; our only two choices are either `'tanh'` or `'relu'` (shorthands for `torch.nn.Tanh` and `torch.nn.ReLU` respectively)\n",
    "\n",
    "Read the documentation (!) for further details.\n",
    "\n",
    "A brief example is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 48])\n"
     ]
    }
   ],
   "source": [
    "rnn = torch.nn.RNN(input_size=16, hidden_size=48, nonlinearity='tanh')\n",
    "X = torch.rand(10, 32, 16)\n",
    "h, _ = rnn(X)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for a random input tensor of shape (seq_len, batch_size, input_dim), we get back an output tensor of shape (seq_len, batch_size, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mySRN, rnn, X, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2.1: A faster version of the SRN\n",
    "Now let's wrap an `RNN` into a custom class `myFastSRN` that implements it aside the `h_to_y` transformation.\n",
    "Complete the class by filling in the missing `h_to_y` transformation and its `forward` pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myFastSRN(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, \n",
    "                 hidden_activation: str,\n",
    "                 output_activation: Callable[[torch.FloatTensor], torch.FloatTensor],\n",
    "                 device: str):\n",
    "        super(myFastSRN, self).__init__()\n",
    "        self.output_activation = output_activation\n",
    "        self.device = device\n",
    "        \n",
    "        self.rnn = torch.nn.RNN(input_size = input_dim, hidden_size = hidden_dim, nonlinearity = 'tanh').to(self.device)\n",
    "        self.h_to_y = torch.nn.Linear(in_features = hidden_dim, out_features = output_dim, bias = True).to(self.device)\n",
    "\n",
    "    def forward(self, X:torch.FloatTensor, h: Optional[torch.FloatTensor]=None) -> torch.FloatTensor:\n",
    "        next_y = self.h_to_y(self.rnn(X)[0])\n",
    "        next_y = self.output_activation(next_y)\n",
    "        return next_y                            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our new implementation in action. We will test it for a random input tensor X of shape (10, 32, 16) (i.e. 32 sequences, each of length 10, with each item having 16 features). We should expect an output tensor Y of shape (10, 32, 5) (i.e. 32 sequences, each of length 10, with each item having 5 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 5])\n"
     ]
    }
   ],
   "source": [
    "rnn = myFastSRN(input_dim=16, \n",
    "                hidden_dim=48, \n",
    "                output_dim=5, \n",
    "                hidden_activation='tanh', \n",
    "                output_activation=torch.nn.Softmax(dim=-1), \n",
    "                device='cpu')\n",
    "# a random input of 32 sequences, each of length 10, with each item containing 16 feature\n",
    "X = torch.rand(10, 32, 16)  \n",
    "print(rnn(torch.rand(10, 32, 16)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully everything should be in order.\n",
    "\n",
    "You may have noticed a minor complication: in order to utilize batching, we need our input sequences to be of the same length.\n",
    "\n",
    "This however is very rarely the case in practice. A common trick against this problem is _padding_; that is, appending zero tensors to all input sequences shorter than the maximum in-batch length to make them all equally long.\n",
    "\n",
    "As usual, torch already does the hard work for us via [pad_sequence](https://pytorch.org/docs/stable/nn.html?highlight=pad%20_sequence#torch.nn.utils.rnn.pad_sequence). Given a list of $N$ 2-dimensional tensors, each of shape (seq_len, input_dim), it will construct a 3-d tensor of shape (max_seq_len, N, input_dim).\n",
    "\n",
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.rand(1, 16)  # a sequence of 1, 16-dimensional item\n",
    "x_2 = torch.rand(7, 16)  # a sequence of 7, 16-dimensional items\n",
    "x_3 = torch.rand(5, 16)  # a sequence of 5, 16-dimensional items\n",
    "\n",
    "X = torch.nn.utils.rnn.pad_sequence([x_1, x_2, x_3])  # What is the shape of X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_1, x_2, x_3, X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Word Embeddings\n",
    "Moving on-- last assignment, we saw how to train our own word embeddings using a miniature toy corpus. Now, we will see how to easily employ high-quality pretrained word vectors and, later on, how to utilize them for further downstream tasks.\n",
    "\n",
    "We are going to use [spaCy](https://spacy.io/). SpaCy is a high-level NLP library that provides a ton of useful functionalities, but we will only focus on its pretrained embeddings for this assignment.\n",
    "\n",
    "Before proceeding, [install spacy](https://spacy.io/usage) using your python package manager (e.g. `pip install spacy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy comes with a lot of different-size models for different languages. \n",
    "\n",
    "We will need to download the small english model for the exercises to follow. You can either do it on a new terminal window (optimal, if you are running this assignment through a virtual environment) or by simply running the magic command below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in c:\\users\\fleur\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having downloaded the model, we can load it as follows (you may need to restart your notebook after the download is complete):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the loaded model to process a sentence and obtain its word vectors, a List of 96-dimensional numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 7 vectors..\n",
      "..each of shape (96,)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('this is a sentence of 7 words')  # the processed sentence\n",
    "vectors = list(map(lambda x: x.vector, doc))  # its vectors\n",
    "print('We have {} vectors..'.format(len(vectors)))\n",
    "print('..each of shape {}'.format(vectors[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then finally convert them into torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 96])\n"
     ]
    }
   ],
   "source": [
    "torch_vectors = torch.tensor(vectors)\n",
    "print(torch_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, in the case of multiple sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example sentences\n",
    "sentences = ['This is a sentence', 'This is another sentence.']\n",
    "\n",
    "# Parallel processing with spacy\n",
    "docs = nlp.pipe(sentences, batch_size=16)\n",
    "\n",
    "# Convert each processed sentence into a list of vectors\n",
    "vectors = map(lambda doc: [word.vector for word in doc], docs)\n",
    "\n",
    "# Convert each list of vectors into a 2-d torch tensor\n",
    "tensors = list(map(lambda sentence_vectors: torch.tensor(sentence_vectors), vectors))\n",
    "\n",
    "len(tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "Given our pretrained embeddings, we may represent sentences as _sequences of vectors_, which is exactly the format expected by an RNN.\n",
    "We will now try to train an SRN to iterate over a sentence and assign part of speech tags to each of its words.\n",
    "\n",
    "(Question: What is the advantage to using a recurrent network over a feedforward network that processes each word individually?)\n",
    "\n",
    "First, let's load and inspect our datafiles.\n",
    "\n",
    "The pickle file contains three items:\n",
    "1. `sentences`: a List of strings (-sentences)\n",
    "1. `postags`: a List of Lists of strings (-POS tags)\n",
    "2. `pos_to_int`: a Dictionary from strings to ints (mapping each POS tag to a unique identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Currently', 'RB'), (',', ','), ('the', 'DT'), ('rules', 'NNS'), ('force', 'VBP'), ('executives', 'NNS'), (',', ','), ('directors', 'NNS'), ('and', 'CC'), ('other', 'JJ'), ('corporate', 'JJ'), ('insiders', 'NNS'), ('to', 'TO'), ('report', 'VB'), ('purchases', 'NNS'), ('and', 'CC'), ('sales', 'NNS'), ('of', 'IN'), ('their', 'PRP$'), ('companies', 'NNS'), (\"'\", 'POS'), ('shares', 'NNS'), ('within', 'IN'), ('about', 'IN'), ('a', 'DT'), ('month', 'NN'), ('after', 'IN'), ('the', 'DT'), ('transaction', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('TRAIN.p', 'rb') as f:\n",
    "    sentences, postags, pos_to_int = pickle.load(f)\n",
    "    \n",
    "print(list(zip(sentences[12].split(), postags[12])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to convert our data to numeric form.\n",
    "\n",
    "Convert sentences to their tensor format, as done earlier (this may take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "docs = nlp.pipe(sentences, batch_size = 16)\n",
    "doc_vectors = map(lambda doc: [word.vector for word in doc], docs)\n",
    "doc_tensors = list(map(lambda sentence_vectors: torch.tensor(sentence_vectors), doc_vectors))\n",
    "# We no longer need the docs and numpy arrays\n",
    "del doc_vectors, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we will use `pos_to_int` to convert the POS sequences into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30197\n",
      "45\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(doc_tensors))\n",
    "print(len(doc_tensors[0]))\n",
    "print(len(doc_tensors[3]))\n",
    "pos_numeric = list(map(lambda pos_sequence: [pos_to_int[pos] for pos in pos_sequence], postags))\n",
    "pos_tensors =  list(map(lambda pos_num_sequence: torch.tensor(pos_num_sequence), pos_numeric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first assignment, we saw how to split our dataset into a training and a validation set. \n",
    "\n",
    "We will do the same here, splitting the sentences, postags and their corresponding tensors into a training and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "sentences_train, sentences_val, postags_train, postags_val, X_train, X_val, Y_train, Y_val \\\n",
    "    = train_test_split(sentences, postags, doc_tensors, pos_tensors, test_size=0.2, random_state=42)\n",
    "assert len(X_train) == len(Y_train) == len(sentences_train)\n",
    "assert len(X_val) == len(Y_val) == len(sentences_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, following along the first assignment, we will wrap our tensors into a `Dataset` and a `DataLoader`.\n",
    "\n",
    "Since our data are not Tensors but rather Lists of Tensors of uneven lengths, we need to write our own Dataset wrapper.\n",
    "The wrapper only needs to implement two functions; `__len__`, which expects no arguments and returns the number of samples in the dataset, and `__getitem__`, which accepts an index `idx` and returns the input-output pair X[idx], Y[idx].\n",
    "\n",
    "Similarly, the Dataloader needs to process the list of input-output pairs produced by the Dataset using `pad_sequence`, as seen earlier.\n",
    "\n",
    "(Question: What is the advantage to applying padding on the batch rather than the entire dataset?)\n",
    "***Then the eventual network can process new sentences without padding as well as sentences with padding.***\n",
    "\n",
    "The code below shows how this can be accomplished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class UnevenLengthDataset(Dataset):\n",
    "    def __init__(self, X: List[torch.FloatTensor], Y: List[torch.LongTensor]) -> None:\n",
    "        super(UnevenLengthDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.FloatTensor, torch.LongTensor]:\n",
    "        return self.X[idx], self.Y[idx]\n",
    "        \n",
    "train_dataset = UnevenLengthDataset(X_train, Y_train)\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              # collate_fn is the post-processing function applied by the dataloader\n",
    "                              # given a batch (a list of i/o pairs),\n",
    "                              # it separately pads all of the inputs and all of the outputs\n",
    "                              # and returns a tuple of two padded tensors\n",
    "                              collate_fn=lambda batch: (pad_sequence([sample[0] for sample in batch]),\n",
    "                                                        pad_sequence([sample[1] for sample in batch])),\n",
    "                              shuffle=True,\n",
    "                              batch_size=32)\n",
    "\n",
    "val_dataset = UnevenLengthDataset(X_val, Y_val)\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            collate_fn=lambda batch: (pad_sequence([sample[0] for sample in batch]),\n",
    "                                                      pad_sequence([sample[1] for sample in batch])),\n",
    "                            shuffle=False,\n",
    "                            batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a batch look like, shape-wise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47, 32, 96])\n",
      "torch.Size([47, 32])\n"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in train_dataloader:\n",
    "    print(batch_x.shape)\n",
    "    print(batch_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good. On to the network.\n",
    "\n",
    "### Assignment 2.2: Utility Functions\n",
    "Remember how we defined our training and validation functions for the first assignment?\n",
    "\n",
    "You will need to do the same here.\n",
    "Note that while you can use the given code as a guideline, just copying it won't do the trick; unlike a feedforward net, a recurrent network produces a 3rd order output tensor, of shape (max_seq_len, batch_size, num_output_classes).\n",
    "\n",
    "Similarly, our target Y is a 2nd order tensor of shape (max_seq_len, batch_size).\n",
    "\n",
    "You will need to properly treat the extra dimensional of both the output and the target, since loss functions expect an order 2 output tensor and an order 1 target tensor. \n",
    "\n",
    "Complete the functions `train_batch`, `train_epoch`, `eval_batch` and `eval_epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions: torch.LongTensor, truth: torch.LongTensor, ignore_idx: int) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "        Given a tensor containing the network's predictions and a tensor containing the true values, as well\n",
    "        as an output value to ignore (e.g. the padding value), computes and returns the total count of non-\n",
    "        ignored values as well the total count of correctly predicted values.\n",
    "        \n",
    "        predictions: The network's predictions.\n",
    "        truth: The true output labels.\n",
    "        ignore_idx: The output padding value, to be ignored in accuracy calculation.\n",
    "    \"\"\"\n",
    "    \n",
    "    correct_words = torch.ones(predictions.size())\n",
    "    correct_words[predictions != truth] = 0\n",
    "    correct_words[truth == ignore_idx] = 1\n",
    "\n",
    "    num_correct_words = correct_words.sum().item()\n",
    "    num_masked_words = len(truth[truth == ignore_idx])\n",
    "\n",
    "    return predictions.shape[0] * predictions.shape[1] - num_masked_words, num_correct_words - num_masked_words\n",
    "\n",
    "\n",
    "def measure_accuracy(network: torch.nn.Module,\n",
    "                    dataloader: DataLoader,\n",
    "                    device: str) -> float:\n",
    "    \"\"\"\n",
    "        Given a network, a dataloader and a device, iterates over the dataset and returns the network's accuracy.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        pred = network(x_batch.to(device))\n",
    "        local_total, local_correct = accuracy(pred.argmax(dim=-1), y_batch.to(device), ignore_idx=0)\n",
    "        correct+= local_correct\n",
    "        total+= local_total\n",
    "    \n",
    "    return correct/total\n",
    "    \n",
    "\n",
    "def train_batch(network: torch.nn.Module,  # the network\n",
    "                X_batch: torch.FloatTensor,  # the X batch\n",
    "                Y_batch: torch.LongTensor,   # the Y batch\n",
    "                # a function from a FloatTensor (prediction) and a FloatTensor (Y) to a FloatTensor (the loss)\n",
    "                loss_fn: Callable[[torch.FloatTensor, torch.FloatTensor], torch.FloatTensor],  \n",
    "                # the optimizer\n",
    "                optimizer: torch.optim.Optimizer) -> float:\n",
    "    network.train()\n",
    "    X_shape = X_batch.shape\n",
    "    print(X_shape)\n",
    "    prediction_batch = network(X_batch)\n",
    "    print(X_batch.view(X_shape[0]*X_shape[1],-1).shape)\n",
    "    print(prediction_batch.view(-1).shape)\n",
    "    batch_loss = loss_fn(prediction_batch.view(X_shape[0]*X_shape[1],-1), Y_batch.view(-1))\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return batch_loss.item()\n",
    "    \n",
    "\n",
    "def train_epoch(network: torch.nn.Module, \n",
    "                # a list of data points x\n",
    "                dataloader: DataLoader,\n",
    "                loss_fn: Callable[[torch.FloatTensor, torch.FloatTensor], torch.FloatTensor],\n",
    "                optimizer: torch.optim.Optimizer, \n",
    "                device: str) -> float:\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss += train_batch(network = network, \n",
    "                           X_batch = x_batch,\n",
    "                           Y_batch = y_batch,\n",
    "                           loss_fn = loss_fn,\n",
    "                           optimizer = optimizer)\n",
    "        loss /= (i + 1)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def eval_batch(network: torch.nn.Module,  # the network\n",
    "                X_batch: torch.FloatTensor,  # the X batch\n",
    "                Y_batch: torch.LongTensor,   # the Y batch\n",
    "                loss_fn: Callable[[torch.FloatTensor, torch.LongTensor], torch.FloatTensor]) -> float:\n",
    "    \n",
    "    network.eval()\n",
    "    X_shape = X_batch.shape\n",
    "    with torch.no_grad():\n",
    "        prediction_batch = network(X_batch)\n",
    "        batch_loss = loss_fn(prediction_batch.view(X_shape[0]*X_shape[1],-1), Y_batch.view(-1))\n",
    "        \n",
    "        return batch_loss.item()\n",
    "\n",
    "def eval_epoch(network: torch.nn.Module, \n",
    "                # a list of data points x\n",
    "                dataloader: DataLoader,\n",
    "                loss_fn: Callable[[torch.FloatTensor, torch.LongTensor], torch.FloatTensor],\n",
    "                device: str) -> float:\n",
    "    \n",
    "    loss = 0.\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss += eval_batch(network=network, X_batch=x_batch, Y_batch=y_batch, loss_fn=loss_fn)\n",
    "    loss /= (i+1)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2.3: SRN POS Tagging\n",
    "Define a simple recurrent network, with input size compatible with the vector dimensionality, output size compatible with the number of output classes (the number of different POS tags + 1) and a hidden size of your own choice.\n",
    "\n",
    "(Question: Why do we need to add +1 to the output size?) ***One node is added in order to represent the possibility that none of the POS-tags fit a given word.***\n",
    "\n",
    "Use `\"tanh\"` as your hidden layer activation, and `torch.nn.LogSoftmax(dim=-1)` for the output activation.\n",
    "\n",
    "Then instantiate an optimizer over your network, and train it for a number of epochs, measuring and printing the validation loss and accuracy at each epoch using the given `loss_fn`.\n",
    "\n",
    "_Hint_: Use `measure_accuracy` (defined earlier) to obtain the validation accuracy.\n",
    "\n",
    "Plot the loss curves over the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myFastSRN(\n",
      "  (output_activation): LogSoftmax()\n",
      "  (rnn): RNN(96, 64)\n",
      "  (h_to_y): Linear(in_features=64, out_features=49, bias=True)\n",
      ")\n",
      "torch.Size([46, 32, 96])\n",
      "torch.Size([1472, 96])\n",
      "torch.Size([72128])\n",
      "Epoch 0\n",
      " Training Loss: 3.9931468963623047\n",
      " Validation Loss: 4.00509811456872\n",
      "torch.Size([38, 32, 96])\n",
      "torch.Size([1216, 96])\n",
      "torch.Size([59584])\n",
      "Epoch 1\n",
      " Training Loss: 3.9818408489227295\n",
      " Validation Loss: 4.003940222755311\n",
      "torch.Size([48, 32, 96])\n",
      "torch.Size([1536, 96])\n",
      "torch.Size([75264])\n",
      "Epoch 2\n",
      " Training Loss: 3.9929986000061035\n",
      " Validation Loss: 4.0027891164103515\n",
      "torch.Size([49, 32, 96])\n",
      "torch.Size([1568, 96])\n",
      "torch.Size([76832])\n",
      "Epoch 3\n",
      " Training Loss: 3.992483377456665\n",
      " Validation Loss: 4.001640792876955\n",
      "torch.Size([46, 32, 96])\n",
      "torch.Size([1472, 96])\n",
      "torch.Size([72128])\n",
      "Epoch 4\n",
      " Training Loss: 4.012219429016113\n",
      " Validation Loss: 4.000493463385042\n",
      "torch.Size([43, 32, 96])\n",
      "torch.Size([1376, 96])\n",
      "torch.Size([67424])\n",
      "Epoch 5\n",
      " Training Loss: 4.001250267028809\n",
      " Validation Loss: 3.999348941934172\n",
      "torch.Size([48, 32, 96])\n",
      "torch.Size([1536, 96])\n",
      "torch.Size([75264])\n",
      "Epoch 6\n",
      " Training Loss: 4.010146141052246\n",
      " Validation Loss: 3.9982029786185613\n",
      "torch.Size([48, 32, 96])\n",
      "torch.Size([1536, 96])\n",
      "torch.Size([75264])\n",
      "Epoch 7\n",
      " Training Loss: 3.975013494491577\n",
      " Validation Loss: 3.997056549819058\n",
      "torch.Size([46, 32, 96])\n",
      "torch.Size([1472, 96])\n",
      "torch.Size([72128])\n",
      "Epoch 8\n",
      " Training Loss: 4.015918254852295\n",
      " Validation Loss: 3.9959143520032288\n",
      "torch.Size([54, 32, 96])\n",
      "torch.Size([1728, 96])\n",
      "torch.Size([84672])\n",
      "Epoch 9\n",
      " Training Loss: 3.9816575050354004\n",
      " Validation Loss: 3.994771224481088\n",
      "torch.Size([44, 32, 96])\n",
      "torch.Size([1408, 96])\n",
      "torch.Size([68992])\n",
      "Epoch 10\n",
      " Training Loss: 3.9709346294403076\n",
      " Validation Loss: 3.99362732115246\n",
      "torch.Size([44, 32, 96])\n",
      "torch.Size([1408, 96])\n",
      "torch.Size([68992])\n",
      "Epoch 11\n",
      " Training Loss: 3.9722185134887695\n",
      " Validation Loss: 3.9924843260850857\n",
      "torch.Size([49, 32, 96])\n",
      "torch.Size([1568, 96])\n",
      "torch.Size([76832])\n",
      "Epoch 12\n",
      " Training Loss: 4.016210556030273\n",
      " Validation Loss: 3.991343826213211\n",
      "torch.Size([58, 32, 96])\n",
      "torch.Size([1856, 96])\n",
      "torch.Size([90944])\n",
      "Epoch 13\n",
      " Training Loss: 3.9703547954559326\n",
      " Validation Loss: 3.9902023852817594\n",
      "torch.Size([65, 32, 96])\n",
      "torch.Size([2080, 96])\n",
      "torch.Size([101920])\n",
      "Epoch 14\n",
      " Training Loss: 3.9885189533233643\n",
      " Validation Loss: 3.9890614893070606\n",
      "torch.Size([47, 32, 96])\n",
      "torch.Size([1504, 96])\n",
      "torch.Size([73696])\n",
      "Epoch 15\n",
      " Training Loss: 3.964298963546753\n",
      " Validation Loss: 3.987919391147674\n",
      "torch.Size([50, 32, 96])\n",
      "torch.Size([1600, 96])\n",
      "torch.Size([78400])\n",
      "Epoch 16\n",
      " Training Loss: 3.994905948638916\n",
      " Validation Loss: 3.9867787499907155\n",
      "torch.Size([61, 32, 96])\n",
      "torch.Size([1952, 96])\n",
      "torch.Size([95648])\n",
      "Epoch 17\n",
      " Training Loss: 3.983323574066162\n",
      " Validation Loss: 3.9856352579025995\n",
      "torch.Size([49, 32, 96])\n",
      "torch.Size([1568, 96])\n",
      "torch.Size([76832])\n",
      "Epoch 18\n",
      " Training Loss: 3.9860305786132812\n",
      " Validation Loss: 3.9844922792343866\n",
      "torch.Size([58, 32, 96])\n",
      "torch.Size([1856, 96])\n",
      "torch.Size([90944])\n",
      "Epoch 19\n",
      " Training Loss: 3.980121374130249\n",
      " Validation Loss: 3.9833495175396956\n",
      "torch.Size([41, 32, 96])\n",
      "torch.Size([1312, 96])\n",
      "torch.Size([64288])\n",
      "Epoch 20\n",
      " Training Loss: 3.9986536502838135\n",
      " Validation Loss: 3.982209248517556\n",
      "torch.Size([50, 32, 96])\n",
      "torch.Size([1600, 96])\n",
      "torch.Size([78400])\n",
      "Epoch 21\n",
      " Training Loss: 3.9911131858825684\n",
      " Validation Loss: 3.981070601750934\n",
      "torch.Size([44, 32, 96])\n",
      "torch.Size([1408, 96])\n",
      "torch.Size([68992])\n",
      "Epoch 22\n",
      " Training Loss: 3.9917476177215576\n",
      " Validation Loss: 3.9799298546301625\n",
      "torch.Size([61, 32, 96])\n",
      "torch.Size([1952, 96])\n",
      "torch.Size([95648])\n",
      "Epoch 23\n",
      " Training Loss: 3.982729911804199\n",
      " Validation Loss: 3.9787879469533447\n",
      "torch.Size([39, 32, 96])\n",
      "torch.Size([1248, 96])\n",
      "torch.Size([61152])\n",
      "Epoch 24\n",
      " Training Loss: 4.022112846374512\n",
      " Validation Loss: 3.97764876784471\n",
      "torch.Size([67, 32, 96])\n",
      "torch.Size([2144, 96])\n",
      "torch.Size([105056])\n",
      "Epoch 25\n",
      " Training Loss: 3.9745402336120605\n",
      " Validation Loss: 3.9765113724602594\n",
      "torch.Size([37, 32, 96])\n",
      "torch.Size([1184, 96])\n",
      "torch.Size([58016])\n",
      "Epoch 26\n",
      " Training Loss: 3.9553792476654053\n",
      " Validation Loss: 3.975372067204228\n",
      "torch.Size([49, 32, 96])\n",
      "torch.Size([1568, 96])\n",
      "torch.Size([76832])\n",
      "Epoch 27\n",
      " Training Loss: 3.988698959350586\n",
      " Validation Loss: 3.9742347790450645\n",
      "torch.Size([55, 32, 96])\n",
      "torch.Size([1760, 96])\n",
      "torch.Size([86240])\n",
      "Epoch 28\n",
      " Training Loss: 3.969783067703247\n",
      " Validation Loss: 3.9731001677336515\n",
      "torch.Size([48, 32, 96])\n",
      "torch.Size([1536, 96])\n",
      "torch.Size([75264])\n",
      "Epoch 29\n",
      " Training Loss: 3.9758551120758057\n",
      " Validation Loss: 3.9719645977020264\n",
      "torch.Size([46, 32, 96])\n",
      "torch.Size([1472, 96])\n",
      "torch.Size([72128])\n",
      "Epoch 30\n",
      " Training Loss: 3.9566292762756348\n",
      " Validation Loss: 3.9708313311218584\n",
      "torch.Size([39, 32, 96])\n",
      "torch.Size([1248, 96])\n",
      "torch.Size([61152])\n",
      "Epoch 31\n",
      " Training Loss: 3.965898036956787\n",
      " Validation Loss: 3.9697009280875877\n",
      "torch.Size([40, 32, 96])\n",
      "torch.Size([1280, 96])\n",
      "torch.Size([62720])\n",
      "Epoch 32\n",
      " Training Loss: 3.950153350830078\n",
      " Validation Loss: 3.9685701037210133\n",
      "torch.Size([74, 32, 96])\n",
      "torch.Size([2368, 96])\n",
      "torch.Size([116032])\n",
      "Epoch 33\n",
      " Training Loss: 3.9517836570739746\n",
      " Validation Loss: 3.9674377491865207\n",
      "torch.Size([54, 32, 96])\n",
      "torch.Size([1728, 96])\n",
      "torch.Size([84672])\n",
      "Epoch 34\n",
      " Training Loss: 3.968698024749756\n",
      " Validation Loss: 3.9663043526745345\n",
      "torch.Size([42, 32, 96])\n",
      "torch.Size([1344, 96])\n",
      "torch.Size([65856])\n",
      "Epoch 35\n",
      " Training Loss: 3.984978437423706\n",
      " Validation Loss: 3.965175148040529\n",
      "torch.Size([54, 32, 96])\n",
      "torch.Size([1728, 96])\n",
      "torch.Size([84672])\n",
      "Epoch 36\n",
      " Training Loss: 3.9734761714935303\n",
      " Validation Loss: 3.964044528032737\n",
      "torch.Size([56, 32, 96])\n",
      "torch.Size([1792, 96])\n",
      "torch.Size([87808])\n",
      "Epoch 37\n",
      " Training Loss: 3.9477908611297607\n",
      " Validation Loss: 3.9629095635086142\n",
      "torch.Size([40, 32, 96])\n",
      "torch.Size([1280, 96])\n",
      "torch.Size([62720])\n",
      "Epoch 38\n",
      " Training Loss: 3.95890474319458\n",
      " Validation Loss: 3.9617741133170155\n",
      "torch.Size([70, 32, 96])\n",
      "torch.Size([2240, 96])\n",
      "torch.Size([109760])\n",
      "Epoch 39\n",
      " Training Loss: 3.959529161453247\n",
      " Validation Loss: 3.9606386795245783\n",
      "torch.Size([64, 32, 96])\n",
      "torch.Size([2048, 96])\n",
      "torch.Size([100352])\n",
      "Epoch 40\n",
      " Training Loss: 3.9806442260742188\n",
      " Validation Loss: 3.9595070755670942\n",
      "torch.Size([45, 32, 96])\n",
      "torch.Size([1440, 96])\n",
      "torch.Size([70560])\n",
      "Epoch 41\n",
      " Training Loss: 3.9560554027557373\n",
      " Validation Loss: 3.95837248948516\n",
      "torch.Size([49, 32, 96])\n",
      "torch.Size([1568, 96])\n",
      "torch.Size([76832])\n",
      "Epoch 42\n",
      " Training Loss: 3.972315788269043\n",
      " Validation Loss: 3.9572371048902077\n",
      "torch.Size([47, 32, 96])\n",
      "torch.Size([1504, 96])\n",
      "torch.Size([73696])\n",
      "Epoch 43\n",
      " Training Loss: 3.9419443607330322\n",
      " Validation Loss: 3.9561008183413713\n",
      "torch.Size([46, 32, 96])\n",
      "torch.Size([1472, 96])\n",
      "torch.Size([72128])\n",
      "Epoch 44\n",
      " Training Loss: 3.941868543624878\n",
      " Validation Loss: 3.9549615912967258\n",
      "torch.Size([53, 32, 96])\n",
      "torch.Size([1696, 96])\n",
      "torch.Size([83104])\n",
      "Epoch 45\n",
      " Training Loss: 3.9614691734313965\n",
      " Validation Loss: 3.9538243271055675\n",
      "torch.Size([47, 32, 96])\n",
      "torch.Size([1504, 96])\n",
      "torch.Size([73696])\n",
      "Epoch 46\n",
      " Training Loss: 3.957653045654297\n",
      " Validation Loss: 3.9526896173991855\n",
      "torch.Size([43, 32, 96])\n",
      "torch.Size([1376, 96])\n",
      "torch.Size([67424])\n",
      "Epoch 47\n",
      " Training Loss: 3.968703031539917\n",
      " Validation Loss: 3.9515563886632363\n",
      "torch.Size([42, 32, 96])\n",
      "torch.Size([1344, 96])\n",
      "torch.Size([65856])\n",
      "Epoch 48\n",
      " Training Loss: 3.957442045211792\n",
      " Validation Loss: 3.9504225657730507\n",
      "torch.Size([55, 32, 96])\n",
      "torch.Size([1760, 96])\n",
      "torch.Size([86240])\n",
      "Epoch 49\n",
      " Training Loss: 3.955401659011841\n",
      " Validation Loss: 3.949292398634411\n",
      "torch.Size([55, 32, 96])\n",
      "torch.Size([1760, 96])\n",
      "torch.Size([86240])\n",
      "Epoch 50\n",
      " Training Loss: 3.9389257431030273\n",
      " Validation Loss: 3.9481622352801935\n",
      "torch.Size([52, 32, 96])\n",
      "torch.Size([1664, 96])\n",
      "torch.Size([81536])\n",
      "Epoch 51\n",
      " Training Loss: 3.9537084102630615\n",
      " Validation Loss: 3.947034027210619\n",
      "torch.Size([58, 32, 96])\n",
      "torch.Size([1856, 96])\n",
      "torch.Size([90944])\n",
      "Epoch 52\n",
      " Training Loss: 3.9433820247650146\n",
      " Validation Loss: 3.945906787953049\n",
      "torch.Size([41, 32, 96])\n",
      "torch.Size([1312, 96])\n",
      "torch.Size([64288])\n",
      "Epoch 53\n",
      " Training Loss: 3.970176935195923\n",
      " Validation Loss: 3.9447759610635265\n",
      "torch.Size([41, 32, 96])\n",
      "torch.Size([1312, 96])\n",
      "torch.Size([64288])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54\n",
      " Training Loss: 3.9385149478912354\n",
      " Validation Loss: 3.943644126256307\n",
      "torch.Size([56, 32, 96])\n",
      "torch.Size([1792, 96])\n",
      "torch.Size([87808])\n",
      "Epoch 55\n",
      " Training Loss: 3.9187912940979004\n",
      " Validation Loss: 3.9425135642763167\n",
      "torch.Size([38, 32, 96])\n",
      "torch.Size([1216, 96])\n",
      "torch.Size([59584])\n",
      "Epoch 56\n",
      " Training Loss: 3.936589002609253\n",
      " Validation Loss: 3.9413846712263805\n",
      "torch.Size([43, 32, 96])\n",
      "torch.Size([1376, 96])\n",
      "torch.Size([67424])\n",
      "Epoch 57\n",
      " Training Loss: 3.956603765487671\n",
      " Validation Loss: 3.94025452931722\n",
      "torch.Size([41, 32, 96])\n",
      "torch.Size([1312, 96])\n",
      "torch.Size([64288])\n",
      "Epoch 58\n",
      " Training Loss: 3.9428765773773193\n",
      " Validation Loss: 3.9391243962383773\n",
      "torch.Size([53, 32, 96])\n",
      "torch.Size([1696, 96])\n",
      "torch.Size([83104])\n",
      "Epoch 59\n",
      " Training Loss: 3.928391695022583\n",
      " Validation Loss: 3.937995303875555\n",
      "torch.Size([45, 32, 96])\n",
      "torch.Size([1440, 96])\n",
      "torch.Size([70560])\n",
      "Epoch 60\n",
      " Training Loss: 3.953219175338745\n",
      " Validation Loss: 3.9368648213684243\n",
      "torch.Size([41, 32, 96])\n",
      "torch.Size([1312, 96])\n",
      "torch.Size([64288])\n",
      "Epoch 61\n",
      " Training Loss: 3.9458167552948\n",
      " Validation Loss: 3.935733305714118\n",
      "torch.Size([42, 32, 96])\n",
      "torch.Size([1344, 96])\n",
      "torch.Size([65856])\n",
      "Epoch 62\n",
      " Training Loss: 3.9383652210235596\n",
      " Validation Loss: 3.934601376296351\n",
      "torch.Size([54, 32, 96])\n",
      "torch.Size([1728, 96])\n",
      "torch.Size([84672])\n",
      "Epoch 63\n",
      " Training Loss: 3.9371330738067627\n",
      " Validation Loss: 3.9334691453863075\n",
      "torch.Size([54, 32, 96])\n",
      "torch.Size([1728, 96])\n",
      "torch.Size([84672])\n",
      "Epoch 64\n",
      " Training Loss: 3.9187984466552734\n",
      " Validation Loss: 3.9323377470490795\n",
      "torch.Size([45, 32, 96])\n",
      "torch.Size([1440, 96])\n",
      "torch.Size([70560])\n",
      "Epoch 65\n",
      " Training Loss: 3.942574977874756\n",
      " Validation Loss: 3.931205355931842\n",
      "torch.Size([61, 32, 96])\n",
      "torch.Size([1952, 96])\n",
      "torch.Size([95648])\n",
      "Epoch 66\n",
      " Training Loss: 3.929319381713867\n",
      " Validation Loss: 3.9300725964642074\n",
      "torch.Size([44, 32, 96])\n",
      "torch.Size([1408, 96])\n",
      "torch.Size([68992])\n",
      "Epoch 67\n",
      " Training Loss: 3.934358596801758\n",
      " Validation Loss: 3.928942060975171\n",
      "torch.Size([46, 32, 96])\n",
      "torch.Size([1472, 96])\n",
      "torch.Size([72128])\n",
      "Epoch 68\n",
      " Training Loss: 3.936232089996338\n",
      " Validation Loss: 3.927809780867642\n",
      "torch.Size([48, 32, 96])\n",
      "torch.Size([1536, 96])\n",
      "torch.Size([75264])\n",
      "Epoch 69\n",
      " Training Loss: 3.94380784034729\n",
      " Validation Loss: 3.926681821308439\n",
      "torch.Size([69, 32, 96])\n",
      "torch.Size([2208, 96])\n",
      "torch.Size([108192])\n",
      "Epoch 70\n",
      " Training Loss: 3.9000322818756104\n",
      " Validation Loss: 3.9255502968238143\n",
      "torch.Size([38, 32, 96])\n",
      "torch.Size([1216, 96])\n",
      "torch.Size([59584])\n",
      "Epoch 71\n",
      " Training Loss: 3.942383050918579\n",
      " Validation Loss: 3.9244185503197726\n",
      "torch.Size([47, 32, 96])\n",
      "torch.Size([1504, 96])\n",
      "torch.Size([73696])\n",
      "Epoch 72\n",
      " Training Loss: 3.9153549671173096\n",
      " Validation Loss: 3.9232872112718207\n",
      "torch.Size([50, 32, 96])\n",
      "torch.Size([1600, 96])\n",
      "torch.Size([78400])\n",
      "Epoch 73\n",
      " Training Loss: 3.9077963829040527\n",
      " Validation Loss: 3.9221526718644237\n",
      "torch.Size([46, 32, 96])\n",
      "torch.Size([1472, 96])\n",
      "torch.Size([72128])\n",
      "Epoch 74\n",
      " Training Loss: 3.915255546569824\n",
      " Validation Loss: 3.9210193775318287\n",
      "torch.Size([49, 32, 96])\n",
      "torch.Size([1568, 96])\n",
      "torch.Size([76832])\n",
      "Epoch 75\n",
      " Training Loss: 3.9015095233917236\n",
      " Validation Loss: 3.919884272984096\n",
      "torch.Size([62, 32, 96])\n",
      "torch.Size([1984, 96])\n",
      "torch.Size([97216])\n",
      "Epoch 76\n",
      " Training Loss: 3.908294200897217\n",
      " Validation Loss: 3.918752769944529\n",
      "torch.Size([50, 32, 96])\n",
      "torch.Size([1600, 96])\n",
      "torch.Size([78400])\n",
      "Epoch 77\n",
      " Training Loss: 3.9223320484161377\n",
      " Validation Loss: 3.917624421851345\n",
      "torch.Size([65, 32, 96])\n",
      "torch.Size([2080, 96])\n",
      "torch.Size([101920])\n",
      "Epoch 78\n",
      " Training Loss: 3.9164414405822754\n",
      " Validation Loss: 3.9164953219196783\n",
      "torch.Size([49, 32, 96])\n",
      "torch.Size([1568, 96])\n",
      "torch.Size([76832])\n",
      "Epoch 79\n",
      " Training Loss: 3.9002227783203125\n",
      " Validation Loss: 3.915366739192337\n",
      "torch.Size([49, 32, 96])\n",
      "torch.Size([1568, 96])\n",
      "torch.Size([76832])\n",
      "Epoch 80\n",
      " Training Loss: 3.891974687576294\n",
      " Validation Loss: 3.914235063330837\n",
      "torch.Size([44, 32, 96])\n",
      "torch.Size([1408, 96])\n",
      "torch.Size([68992])\n",
      "Epoch 81\n",
      " Training Loss: 3.917729377746582\n",
      " Validation Loss: 3.9131025498506253\n",
      "torch.Size([42, 32, 96])\n",
      "torch.Size([1344, 96])\n",
      "torch.Size([65856])\n",
      "Epoch 82\n",
      " Training Loss: 3.919621706008911\n",
      " Validation Loss: 3.9119691495542175\n",
      "torch.Size([46, 32, 96])\n",
      "torch.Size([1472, 96])\n",
      "torch.Size([72128])\n",
      "Epoch 83\n",
      " Training Loss: 3.9108097553253174\n",
      " Validation Loss: 3.9108376313769626\n",
      "torch.Size([38, 32, 96])\n",
      "torch.Size([1216, 96])\n",
      "torch.Size([59584])\n",
      "Epoch 84\n",
      " Training Loss: 3.8985655307769775\n",
      " Validation Loss: 3.909701113978391\n",
      "torch.Size([41, 32, 96])\n",
      "torch.Size([1312, 96])\n",
      "torch.Size([64288])\n",
      "Epoch 85\n",
      " Training Loss: 3.9410817623138428\n",
      " Validation Loss: 3.908566989595928\n",
      "torch.Size([44, 32, 96])\n",
      "torch.Size([1408, 96])\n",
      "torch.Size([68992])\n",
      "Epoch 86\n",
      " Training Loss: 3.9272499084472656\n",
      " Validation Loss: 3.9074363216521246\n",
      "torch.Size([59, 32, 96])\n",
      "torch.Size([1888, 96])\n",
      "torch.Size([92512])\n",
      "Epoch 87\n",
      " Training Loss: 3.8878448009490967\n",
      " Validation Loss: 3.906304465399848\n",
      "torch.Size([41, 32, 96])\n",
      "torch.Size([1312, 96])\n",
      "torch.Size([64288])\n",
      "Epoch 88\n",
      " Training Loss: 3.9194881916046143\n",
      " Validation Loss: 3.905172556165665\n",
      "torch.Size([44, 32, 96])\n",
      "torch.Size([1408, 96])\n",
      "torch.Size([68992])\n",
      "Epoch 89\n",
      " Training Loss: 3.9060215950012207\n",
      " Validation Loss: 3.904041017804827\n",
      "torch.Size([41, 32, 96])\n",
      "torch.Size([1312, 96])\n",
      "torch.Size([64288])\n",
      "Epoch 90\n",
      " Training Loss: 3.9160749912261963\n",
      " Validation Loss: 3.9029138201758977\n",
      "torch.Size([37, 32, 96])\n",
      "torch.Size([1184, 96])\n",
      "torch.Size([58016])\n",
      "Epoch 91\n",
      " Training Loss: 3.900062084197998\n",
      " Validation Loss: 3.9017862668113104\n",
      "torch.Size([51, 32, 96])\n",
      "torch.Size([1632, 96])\n",
      "torch.Size([79968])\n",
      "Epoch 92\n",
      " Training Loss: 3.8918509483337402\n",
      " Validation Loss: 3.900658646588603\n",
      "torch.Size([46, 32, 96])\n",
      "torch.Size([1472, 96])\n",
      "torch.Size([72128])\n",
      "Epoch 93\n",
      " Training Loss: 3.892019033432007\n",
      " Validation Loss: 3.8995340778714134\n",
      "torch.Size([60, 32, 96])\n",
      "torch.Size([1920, 96])\n",
      "torch.Size([94080])\n",
      "Epoch 94\n",
      " Training Loss: 3.8928632736206055\n",
      " Validation Loss: 3.8984102218870134\n",
      "torch.Size([61, 32, 96])\n",
      "torch.Size([1952, 96])\n",
      "torch.Size([95648])\n",
      "Epoch 95\n",
      " Training Loss: 3.911332607269287\n",
      " Validation Loss: 3.8972853769070257\n",
      "torch.Size([65, 32, 96])\n",
      "torch.Size([2080, 96])\n",
      "torch.Size([101920])\n",
      "Epoch 96\n",
      " Training Loss: 3.870419502258301\n",
      " Validation Loss: 3.8961548477253585\n",
      "torch.Size([60, 32, 96])\n",
      "torch.Size([1920, 96])\n",
      "torch.Size([94080])\n",
      "Epoch 97\n",
      " Training Loss: 3.8731632232666016\n",
      " Validation Loss: 3.8950243841403376\n",
      "torch.Size([37, 32, 96])\n",
      "torch.Size([1184, 96])\n",
      "torch.Size([58016])\n",
      "Epoch 98\n",
      " Training Loss: 3.8924219608306885\n",
      " Validation Loss: 3.893897564953597\n",
      "torch.Size([53, 32, 96])\n",
      "torch.Size([1696, 96])\n",
      "torch.Size([83104])\n",
      "Epoch 99\n",
      " Training Loss: 3.890448570251465\n",
      " Validation Loss: 3.892773109769064\n"
     ]
    }
   ],
   "source": [
    "# Your training script here\n",
    "srn = myFastSRN(input_dim=96, \n",
    "                hidden_dim=64, \n",
    "                output_dim=49, \n",
    "                hidden_activation='tanh', \n",
    "                output_activation=torch.nn.LogSoftmax(dim=-1), \n",
    "                device='cpu')\n",
    "\n",
    "print(srn)\n",
    "opt = torch.optim.Adam(srn.parameters(), lr=1e-05)\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss(ignore_index=0)  # ignore index 0 (misclassification of padded outputs incur no loss)\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "device = \"cpu\"\n",
    "\n",
    "for t in range(NUM_EPOCHS):\n",
    "    train_loss = train_epoch(srn, train_dataloader, optimizer=opt, loss_fn=loss_fn, device=device)\n",
    "    val_loss = eval_epoch(srn, val_dataloader, loss_fn, device=device)\n",
    "    \n",
    "    print('Epoch {}'.format(t))\n",
    "    print(' Training Loss: {}'.format(train_loss))\n",
    "    print(' Validation Loss: {}'.format(val_loss))\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021133792295737957\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your plotting here\n",
    "print(measure_accuracy(srn, dataloader = val_dataloader, device = device))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2.4 (BONUS): Showtime\n",
    "A portion of the dataset has been held out from you. \n",
    "\n",
    "Show off your machine learning skills by designing and training your own network and competing for the best accuracy on the secret test set.\n",
    "\n",
    "Some ideas to try:\n",
    "1. Different recurrent network architectures ([LSTM](https://pytorch.org/docs/stable/nn.html#lstm), [GRU](https://pytorch.org/docs/stable/nn.html#gru))\n",
    "2. Different hyper-parameters (Bi-Directionality, Recurrent Depth, Optimizer and Learning Rate)\n",
    "3. Regularization ([Dropout](https://pytorch.org/docs/stable/nn.html#dropout-layers), Weight Decay)\n",
    "4. Different spaCy models\n",
    "\n",
    "Feel like doing something more advanced? Get in touch ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom model and its functions here\n",
    "\n",
    "NotImplemented"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
