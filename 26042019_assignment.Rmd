---
title: "Lab assignment 1: Image recognition using deep networks"
author: "Priyanka Singhvi and Fleur Petit"
date: "26 April 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("keras")
library("tidyverse")
library("knitr")
```

# Exercise one: Identifying handwritten numbers

## Question 1

Automatic recognition of hand-written numbers can be useful for digitalising any written source that contains numbers. There are many examples: student numbers on exams, hand-written spreadsheets, phone numbers, dates etcetera.

Load the data:

```{r}
mnist <- dataset_mnist()
```

## Data preparation

Use `array_reshape` and `nrow` to convert the train and test set from from 28x28 pixels to a column of 784 pixels for each image.  These should have dimensions 60000x784 and 10000x784 respectively.

```{r}

x_train <- array_reshape(x = mnist$train$x, dim = c(60000, 784))
nrow(x_train)
ncol(x_train)

x_test <- array_reshape(x = mnist$test$x, dim = c(10000, 784))
nrow(x_test)
ncol(x_test)

```

Divide each variable by 255 to scale them to values between 1 and 0.

```{r}

# Test if dividing whole array leads to division of individual numbers.
(test_array <- 1:30)
(test_array/3)
# Works

x_train <- x_train/255
x_test <- x_test/255

```

Use `to_categorical` to convert train and test labels to categories. Each number is represent as an array of nine 0s and one 1.

```{r}

y_train <- to_categorical(mnist$train$y)
y_test <- to_categorical(mnist$test$y)

# Check it out
y_train %>% as_tibble %>% head(5) %>% kable

```

## Model definition

The pixels of each 28x28 pixels image have been reshaped into a list of 784 values between 0-1 indicating the activation of each pixels. The list of pixels is fed to 784 input nodes that are fully connected to a 256 node hidden layer. The softmax function transforms the output of each picture to the probability of it being each of the digits 0-9.

```{r}

model <- keras_model_sequential()
model %>%
  layer_dense(units = 256, input_shape = c(784)) %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model)

```

Compile the model

```{r}

model %>% compile(
 loss = 'categorical_crossentropy',
 optimizer = optimizer_rmsprop(),
 metrics = c('accuracy')
)

```

## Training and evaluation

## Question 2

See output at `Xs Yus`/sample where 

```{r}
history <- model %>% fit(
 x_train, y_train,
 batch_size = 128,
 epochs = 12,
 verbose = 1,
 validation_split = 0.2
)
```


Make a dataframe of the history and look what is in it.

```{r}

history_tb <- history %>% as_tibble

history_tb %>% kable

```

## Question 3

Plot history

```{r}

plot(history)

```

## Question 4

The model performs similar on training and out of training data. It generalises well.

## Question 5

```{r}

score <- model %>% evaluate(
 x_test, y_test,
 verbose = 0
)

score %>%
  as_tibble %>%
  kable


```

## Changin model parameters



## Question 6

The accuracy is not high enough for applications that can have big concequences when the highest precision is not achieved. However, for less precise tasks the accuracy may suffice. 


## Question 7

Keras dense layers use a linear activation function at default. This means that there is no threshold activation. A ReLu function does induce some kind of threshold. I.e. input below a certain value does not activate a neuron. Using this threshold allows one to ignore input that is too small to be relevant, to make the categorisation more efficient. Features of the picture that stand out will be emphasised, while features at the background will be ignored. If node activation is determined by a linear function the prominent features in the picture lack useful emphases.

## Question 8

```{r}

model_relu <- keras_model_sequential()
model_relu %>%
  layer_dense(units = 256, input_shape = c(784), activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

summary(model_relu)

```

```{r, results="hide"}
model_relu %>% compile(
 loss = 'categorical_crossentropy',
 optimizer = optimizer_rmsprop(),
 metrics = c('accuracy')
)

history_relu <- model_relu %>% fit(
 x_train, y_train,
 batch_size = 128,
 epochs = 12,
 verbose = 1,
 validation_split = 0.2
)
```

```{r}
plot(history_relu)
```

## Question 9

The performance on the validation set is a lot lower than the performance on the trainingset. Probably this model does not generalise as well as the previous model.

## Question 10

Probably this will perform worse on the test set than the previous model. This can be expected from the performance on the validation set.

```{r}

score_relu <- model_relu %>% evaluate(
 x_test, y_test,
 verbose = 0
)

score_relu %>%
  as_tibble %>%
  kable


```

Reshape `mnist$train$x` to a new variable (x_train) of size 60000, 28, 28, 1. Reshape `mnist$test$x` to a new variable (x_test) of size 10000, 28, 28, 1. Rescale both results to values between zero and one as before. 

```{r}

x_train <- array_reshape(x = mnist$train$x, dim = c(60000, 28, 28, 1))
nrow(x_train)
ncol(x_train)

x_test <- array_reshape(x = mnist$test$x, dim = c(10000, 28, 28, 1))
nrow(x_test)
ncol(x_test)


x_train <- x_train/255
x_test <- x_test/255


```

Deep Convolutional Neural Net:

  - 2 convolutional layers
  - 32 convolutional filters into the first layer
  - 64 convulational filters into the second
  - 3x3 pixel filters
  - ReLu activation
  - Pooling to downsample size 2nd layer 2 a quarter of the pixels.
  - Flatten to 1-dimensional array and use fully-connected layer to link network to labels.
  
```{r}
model_cnn <- keras_model_sequential() %>%
 layer_conv_2d(filters = 32, kernel_size = c(3,3),
 activation = 'relu', input_shape = c(28,28,1)) %>%
 layer_conv_2d(filters = 64, kernel_size = c(3,3),
 activation = 'relu') %>%
 layer_max_pooling_2d(pool_size = c(2,2)) %>%
 layer_flatten() %>%
 layer_dense(units = 128, activation = 'relu') %>%
 layer_dense(units = 10, activation = 'softmax')

summary(model_cnn)

model_cnn %>% compile(
 loss = 'categorical_crossentropy',
 optimizer =  optimizer_adadelta(),
 metrics = c('accuracy')
)

history_cnn <- model_cnn %>% fit(
 x_train, y_train,
 batch_size = 128,
 epochs = 6,
 verbose = 1,
 validation_split = 0.2
)
```

## Question 11

```{r}

plot(history_cnn) +
  geom_line()

```

## Question 12

The performance on the trainingset and the validationset are very similar for this model. It is generalises better than the previous model, because it performs similar on training data and out of training data.

## Question 13

```{r}

score_cnn <- model_cnn %>% evaluate(
 x_test, y_test,
 verbose = 0
)

score_cnn %>%
  as_tibble %>%
  kable


```

## Question 14

The accuracy is sufficient for automatic hand-written digit classification in applications for which 1 mistake in a 100 digits is acceptable. For example postal codes, phone numbers, age etcetera.

## Question 15

Applying dropout in the trainingstage means that nodes are dropped with a certain probability. In this manner the chances of good performance on training data and worse performance on test data (overfitting) are reduced.

```{r}
model_cnn <- keras_model_sequential() %>%
 layer_conv_2d(filters = 32, kernel_size = c(3,3),
 activation = 'relu', input_shape = c(28,28,1)) %>%
 layer_conv_2d(filters = 64, kernel_size = c(3,3),
 activation = 'relu') %>%
 layer_max_pooling_2d(pool_size = c(2,2)) %>%
 layer_dropout(rate = .5) %>%  
 layer_flatten() %>%
 layer_dense(units = 128, activation = 'relu') %>%
 layer_dense(units = 10, activation = 'softmax')

summary(model_cnn)

model_cnn %>% compile(
 loss = 'categorical_crossentropy',
 optimizer =  optimizer_adadelta(),
 metrics = c('accuracy')
)

history_cnn <- model_cnn %>% fit(
 x_train, y_train,
 batch_size = 128,
 epochs = 6,
 verbose = 1,
 validation_split = 0.2
)

```



## Question 16

```{r}

plot(history_cnn) +
  geom_line()

```

History of the training and validation sets are even closer than in the previous models. Training time was about 10 seconds longer on average.

## Question 17

The models should generalise well, the 2nd one slightly better than the first.
